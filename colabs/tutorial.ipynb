{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx1DwXpPEWeL"
      },
      "source": [
        "Copyright 2024 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SytPbrqtkUF"
      },
      "source": [
        "![onetwo_logo_horizontal.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAhMAAAB4CAYAAABID3CUAAAhbElEQVR4Xu2dC5QU1ZnHm0RUwHA0gmLiLrhGk0AIa1YjxE101VVJYoBE4mO6W4TFVY66Zt1djWYVCPZrjKAGsoiAnLAxEXyhMN09JGOiQiQclBiyJD6QGMCICIjCTFXN1N6vu6uo+er2o7rr1ZP/75z/Aefe76uyqKn77657vxuJAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKBmBgqNEbpY6GqhuyDIZ80Q+rbQaKGPRgAAADQFQ4WmCq0SOiikQ1BItEfox0IXRAAAAISSY4VSQoci9oc4BIVNm4UuiQAAAAgNN0aKn/r4AxuCwq4VkeK3aQAAAALiaKHlEfsD2tSoUaP0iRMn6tddd50+c+ZMCPJV06dP1y+55BJ92LBhtnvTom1CIyMAAAB850ShFyP2B7N+2mmn6XPnztXfeOMNHYAwoGmans/n9ZaWFv0jH/mI7Z4V2id0YfHWBgAA4Af0jYTNSAwdOlRfsGCBrigKf5YDEBo2btyon3322dxMkJQIDAUAAPiG7dXGWWedpe/YsYM/twEIJT09Pfptt93GzQRpbwSvPAAAwHNosmWvB/CkSZP0gwcP8uc1AKHnoYce0o844ghuKLYJDTHveAAAAK5Cyz97rdqgbyQOHTrEn9EANA1kKKz3dEmLzbseAACAq6QjlgfuCSecgFcboE9w6623cjPRLTT28K0PAADADWgtfq+KlvPnz+fPZACaEppDIZmU+RuhfuZvAAAAgIahEtnmg5aWf2LVBuhL0CoPybLR8y2/AwAAABrkqYjlIUt1JADoa0SjUW4maC8PAAAALkC7f/Z6xYGCVP4wa9asQiXH2bNn8ybgAVTYynqfC70bwW6jAADgCp+PWB6wVCIb+INxzY866ijeBDyAKmVKSm+PMn8TAAAA1A3trmg+XKmuBPAeVVXNaz5w4EDeDDxi/Pjx3Ex82/xNAAAAUDfXRCwP1xkzZvDnL/CAzs5O85oPHjyYNwOPuPbaa7mZuMH8TQAAAFA3t0UsD1d6jw+854MPPjCv+XHHHcebgUfQHBXr/S70XfM3AQAAQN3cEbE8XOfMmcOfv8AD9u/fb15z2kQN+APd39b7vXT/AwAAaBCYiQDYs2ePec1pUiDwB5gJAADwBpiJAHjnnXfMa37yySfz5j4FvdJpa2vTlyxZUnjNsHjxYj2XywWy7wvMBAAAeAPMRADs3LnTvObDhw/nzX2C5557Tp8wYYJ+5JFH8gG8oAEDBhQm/O7atYuHegbMBAAAeAPMRAC89dZb5jU/9dRTeXNTs2/fPj0ej/NBu6yGDBmid3R08DSeADMBAADeADMRANu2bTOv+emnn86bm5bdu3frY8aM4QN2VfXv398XQwEzAQAA3gAzEQCvvfaaec1HjhzJm5uS7u5ufdy4cXywrlnHH3984fWPl8BMBICei52g5eJfV7Kxm7Vs7IdCS5V8/GdKLvZT+jv9rNgW/ZreceUQHg8AaApgJgJg69at5jUfPXo0b25K5s2bxwdqx/K6aBrMhE90tbeMVnPRjJqPbVFzMd2hXhFKdLXHPsvzAm9R08nVaibZEZS01uQkfk6gaYCZCIAtW7aY1/yMM87gzU1HT0+PPmLECD5QOxZNyvRylQfMhMdo2fgEYSA2SAxCvfqVmo1SzX/gA8JMvC+kByUlnURJ2uYFZiIANm/ebF7zM888kzc3HevXr+eDdN1qb2/n6V0DZsIjlGz8i2Lg3ygxAy4p+nxXW5x2JQQeosJMgPqBmQiATZs2mdd87NixvLnpWLZsGR+k69bChQt5eteAmXAZfeO1/dVsfJ4Y8LvtBsB1qULf1x+djH3jPUKFmQD1AzMRABs2bDCv+TnnnMObm47W1lY+SNetdDrN07sGzISL6GtaThaD+4uSQd9b5WMdNKmTnw9oHBVmAtQPzEQAWF8LnHvuuby56Xj44Yf5IF23HnzwQZ7eNWAmXKJzbfx0JRfbbhvoa9fekvjPa1M29gd99VXD+XmBxlBhJkD9wEwEAFWHNK75+eefz5ubjnXr1vFBum7l83me3jVgJlygc83Vp4oB/W3bAF9W8d+Kwf9OLR/7qr7mmqE8Hy0H1XLx8Wo+/t9qLvqSPV4uMjP07QjPB+pHDOi/09LJbbVK9N/BDYFFB3n/6kpczc8JNA0wEwHw7LPPmtf8oosu4s1NB9WYOOmkk/hA7VhUepv28vAKmIkGoYFfmIM/8oFdom6qI1HPpMmuXOxzSjb+E5FDk+TlekXvmHIszwH8oav17s9LTERRqWQ77w/6NDATAbB27Vrzmo8fP543NyV33HEHH6gda+LEiTytq8BMNICuR/qp2VhWMqBzbVay0S/weKdQDsolyc/1JI8F/gAzASzATAQA7ZppXPNLL72UNzcl77//vn7iiSfywdqR6PWPl8BMNICaj/+7ZCDvJS0XW0grPHhsvVAukfNBfhwuJRefwWOB98BMAAswEwGwevVq85p7/WncT6hGxBFHHMEH7JpEm4N5DcxEnejt8b8Vg/aHfBDvpWzsTh7nFmouPtN2vN7ar6+eMozHAW+BmQAWYCYCYNWqVeY1v+yyy3hzU7N06VLHhoI2B9u7dy9P5TowE3VC8x8kA7hV9/EYt6E9PCTHNaXl40t4DPAWmAlgAWYiAB5//HHzml9++eW8uemhbyhqfeVBm4PRbqN+ADNRB11rWkaqlYtS/UKfOfMjPM5tqFiVSuW17cc3pB7KxU7hcW6gZ6d9vFgqPHq7OM59tEGZmo2lhcn6jpqL/rOeiw3iMV5AS3K1XOwKccy71Fy8tXge0dm0EkbLRr/VlZvyGR7jJTAT7qDff/9RSjJ5hpJOTBPX7i6he7RUcqmaSswSf79DSycm6vfMGc7jQgbMRACsWLHCvOYtLS28uU+wf/9+/fbbb9eHDRvGB/CChg8fXtgcTNM0HuoZMBN1oOSiD0sGbkMHvBrAZXRmo59SK7xuEQPt/TyGY5gBm/LRK+19r/6KGKifETEKPxbTocJOqO3xM3iORim9YppTa12PQr9s9Aed+atO47ncxi0zIfrPpsFTqjqXi6qp1JdsuUoSx8vw/rUgTPMxPJdVncnk3/GYSmip1KVKOvkTcT4f2K6fRFo6+Spdq0PJ5AieKwTATATAI488Yl5zP+YKBAktG/31r39dKGxF9xf9SRVAgwBmwiH6mpbBYoA6yAesw4rexWO8Rs3G77afh6n39DU3HsVjrIg+f5HEkeYafajCpvh/WyHpU00aDeR6x3lHWI9ZD8VrH28VOTslx6lFKpkkL7d1d81MpJL32uIP6xXevxbE4L5YkstQTz2f9IWx+YYkl6H39IULa5p8LEzEeNH/ZUmOWtWlZRIL9ETCVrclQJrOTFgHp9mzZ+uLFy8urI7wcrdJt1m+fLl5zadOncqbgUfATDhEyUavkQxShvbrz1x1HI/xmmKti9gByfkUpOWiE3mMFbWKmaDt08Wn+z9J2p3oyUYMBS2L1XKx1yR569FONd9yAT+GG7hmJjKJL9viD6tH/8Gcv+ExldAfffSjIu4dSS5TSjpxE4+rhpZO/ojnMfOlkst4f46eTn9MS6eW8NgG9BcyOPw4AdE0ZqLa1+a0ffWMGTP0Xbt28dDQYS0/PX36dN4MPAJmwiHi0/njksGpIFoGyvv7hTj2Yn4+5nll44t4fytqBTNReo1Srt2Z8rF7+LFrQctGvybiD9nyNSZFy8cm82M1iltmgubciJi3bTlKUjLJa3lMJdRU6is8h02ZZAePq4aSTr5py1OSlkpN4P2tCCPxCdHvtzzOFWVSt/PjBUBTmAknE/qGDBmid3R08BShgr5NMc73+uuv583AI2AmHFAqUrVbMjAV1R67mMf4RWnAtZ8TKRv7A+9vRS1jFmg1iPjzN/znDahbaW8Zy49fCSo5LuK6JLnckEaTRfkxG8EtM0GIT/0LbTlMpZ7g/Ssh+v/AnsMmVU8kjuex5ei6JzFSksPQAf3eewfwGAMyEqXy4zzOPWVSaX5cnwm9mahnqSGVZaZXIWGFNrMyzvWGG27gzcAjYCYcQBP4JAOSoS790clH8hi/0DumHK2WnxTZU+n1i1rGTFSSlotto91KxWC8Rvz3C2qFSaC9lI2v5ccvB63UUHPxfbYcvUVG4wklF7uJXl2obdHz1Gz0kuJ/1zRR9F039zNx00yomeQlthyH9b4+c2bN95sYuF+X5LDJyeRO0f8WHm/Ro7y/gTjvgeL/baMkppz+SN+aiD+fEfql0LuSPlIpqdS/8uP7SKjNRCNFkE455ZRCVcYwsmDBAvM8b775Zt4MPAJmwgGlT8l8MDK0mff3G3EOv5OcV0FKW8tZvL+B6sBM0B4htFcIz0FGSsvFLxN9fs9juDrbop/m8ZzCstdsbBOP7XUuudhyPX/FJ3isldImbCt5bK88+fjPeFy9uGkmyCyIuH22PKbu/iceI0MMqGPsseVU+zceYoD/uT2+KC2VuoL3NxDGZj7vz6Wkk39S0omb9bvvPonHE0oy+Q+l+RqdPJZJoX8THu8ToTUTbpRnvu+++3jaUPDAAw+Y53jLLbfwZuARMBMOUHLxG/lAZA5IYpDl/f2GBkV+XobEQH8572+g1mYmFC0fj/JYjr5u8oAaCnp9l8dxKl1rlV6XZGPTeUwlRP+bKU6Si9Tj9PVLOdw0E4QYVJfb8hxWTcs51UzqTklsOX1I3xzwHByaOCn6dkniSZ3UzmMIWp4q2rslMYZ6xPm2inM4msfK6EynP62mEy9K8hxWJrnBj7ovEkJrJtzYOGrUqFE8bSiYO3eueY633norbwYeATPhgOKW4LaBqCAx0D7A+/uNlovO5+dlSBiNsl/3qjWYCS0bi/G4ctD+ISLmZZ7DojYeY4UMiejzriSuIDIaPKYWioWt7PmKOWOP8P714LaZ0DLJb9nyHFZNS0RFv02S2LKqNnGSoKJRPM6ip3l/A9H2rKS/oW4lk5zCY6pBczPomJJ8pip9U+IhoTQTbm1pTdq5cydPHzitra3m+dEKFeAPMBMOoAqPfBAylY3fzfv7jTiPpO28zPOL3cL7G6hVzIQYaKsu8eMIczWJ57Hoz7y/FTI+khhDZQeqahQm0JY3OV2V5pXUittmQjwZB4nYg7ZcRVVdIkq1I6ifJPYv9Ild8nOdik3xPJxKk0OVdOIa3p9QWxPjeF+mqt9YlaMwD6Oyadosnnf9eJzHhNJMrFu3jj/069ZTTz3F0wdOKpUyz+/OO+/kzcAjYCYcID7ZZiSDkKHv8/5+U7l4VfQ/eH8DtbKZOFRtXoKM0oTQcsW9uivVnBDtL0piCnG1zLeoBC0HleQtSMtGv837O8VtM0HQPAZbrpKqLRGl2hE8hiTMwCIlnbyO/7ykd6kuBc9lRcRul8SRyq4I0TKJ/5H0N/RCo68ixLX/nMijSHIXpCSTZecNeUQozYS1DkOjosmOYcM6qM2aNYs3A4+AmXBApa/JVR829qpGpY2/lFz0et7fQK1gJkTcj3n/Wqk0gZL29uD9CT0bPUm09/D+JVV8PVILpVco0pUnbtQJ8cJMaJlU3JbLVOUJk6L9F/YYYSYyia/S5Ea13PyFZPI8nsugNGjbY4rnI12tU6qbscvev6RU6is8ph6qFMBK8P4eE0ozYX0N0KjS6TRPHzhkIIzzC8s1/2sAZsIBpUl8tkGI1Mig6xZKLva//LzMgTIbu4r3N1ArmAmtLfZN3r9WSstGbTlJ5bZHp/PkfS2641DblBGNSs3HtkhyC0Wf5+fjFC/MhJ5MHqeW/8RddokofUMg2lVbTCq5nzbSoj7iv5+3tRf7zOP5DET7f9r6l6SkkzN4f6Irnf4s72vR73n/elFaE2dL8htaz/t7TCjNxLJly/hDv24tWrSIpw8cerVhnB+98gD+ADPhgMIumbYByNRG3t9vxGD4kuS8jIHyS7y/gVrBTOg/j3+S968VEf8kz2fmzUalS/7UfDzF+/qmbGw3Px+neGEmCDWdyNvymZIvEaWaEfa+hQH/p0YftUytCCooZc1lpdy3HULdVIyK9ye0TPJySf+iMqlW3r9exDOtn1q+bPiBRl+lOCSUZmL9+vX8oV+31q5dy9MHDk26NM6PvoUB/gAz4YDS1uP2Qaiog5XmAXhNaQVF2Q2waKMuHmOgljcTCu/rBLUeM5GLPcH7+iiNJmnyc3KCV2ZCSaWut+U7LOkS0XJzLWhgN/rQrp683RBtAW7NR1RZErqO9zcQhuF7kv7F80mlqi45dgJdZ34MQ+L8XStQVgOhNBM9PT36iBEj+IPfsQYOHBjKDcBoOahxjrRMFPgDzIQDCoWUcrH9kkGoqLboeTzGL9Rs/ELb+ZREW3Dz/lbU8mZiL+/rBLUeM5Gnqpr2/n5JmK5B/Jyc4JWZqDi/QbJEtLS64UNJX1sNCGE6XpL0I8209iO01uQkSb+iUonyk3zTyXts/Q1VmJ9RD6Ut1e3HEaL5Hry/h4TSTBDz5s3jD37HmjZtGk8bCqhQlXGOVMAK+APMhEMqzQPQcrH7eX+/EMdewM/HULX5HGp5M/Ee7+sEtR4zkYtt5H39lN4x+Rh+Tk7wykwQIscLtpxF2ZaIUq0IST/SM9Z+RIWiVi/zvlo69aCkX0H0LQfvb1BpJYeSTp/J+zeCONYP+TEsx3KlOFmNhNZMUK2JcePG8Yd/zRo0aJC+fft2njYUUAlt4zzDuNqkrwIz4RAlG72BD0AWvdvoYFQP+pqWweLYeyXnU5CWi7bwGCtqqMxE9Hne10dp9O0TPycneGomxCd/W05jkGRLRMt9OlfSiWnWfkSl1RncIFCZa96nqNRL1n4c0T7XHlNSJvFl3r8RhOF5yHaMkqi0OO/vIaE1E8Tu3bv1MWPG8AGgqvr166evXLmSpwsNtLmXca606RfwB5gJh9DcAzHoqJKBqKTy9Ry8Qs1Hb7efh6kPqxkcNVxmotw3Pwrts8FXZrgpPXdNxQJQteClmejMZE615TR1eIko1YgQP9tt75PU9NZW6dwZlTbTsvcnk/Ido09XKjWat1v039Z8HNE+UxJTEFX55P0bQc0kV/FjGDqUTI7g/T0k1GaC2Ldvnx6Px/kgUFbHHHNMqI0EQduOG+dL25EDf4CZqAO18iTBPeWWPXoBrbZQK3wrUUv1SjVEZkLLxhfxvoY6s9FP8f5hw0szQYg8L9vyFmUuEaU5CJJ20i95PgPRlpL07xWjphL/JWkvSBiNUdZ8HKqKyWMsuov3b4QKW5sr5ZbRekTozYTBc889p0+YMKGwvbj1nA3RZMupU6fqb775Jg8NHdOnTzfPmwp0AX+AmagDtS0+jg90vRV9nMd4hTje0/bjm+rpaotX3TFRDZGZUPLRf+N9DSnZqLRMc5jwwUzcZctrKJk8v9AnlZxna0sXXnHczPMZVKjPoOmJxFDqU9oGnLeTtvJ8HNrlUxJnqOH6HgadmcxpkvyGfsv7e0zTmAmDDz74QG9ra9OXLFkifNdMfenSpXo2mw3lqo1ykOkxrvny5ct5M/AImIk6EYNbng92THXvM1Ar4hjfkxzXqid5jAw1RGZCzV99Du9r0WreP2x4bSYq5i8tES3zybyn0lf8erE+w1uSuMJeG/r9Mwer5QtnVa0sSUWyRL/3JbGkbjIBPKYe1FTi+5L8BdF+Iry/xzSdmegLWF/bPPLII7wZeATMRJ10Za8eJQY3RTLgGepR8rEpPM4tlFx8Kh1DclxDnbW+FlBDZCZKy2/f4f1L6u7KxRpe2tfV3jK6M3+VK4MXp+Jg74KZIMSg+Kotd1GvUG0Iyc9Jm3gejsj7gCSO9JSWTnxT8vOCal2NIUzJz3js4RzJ5by/U+gbFDWd2stzm8okL+ExHgMzEQAtLS3mNV+xYgVvBh4BM9EANVRr7KHJkTyuUUoTLisZCZ32EeFx5VBDZCYI2iOD97foV41UMdSfvnagyPF7IU3Jxn9CppD3aQQ/zITIlbHlLomqW/KflVRxgiRBr0kkcaSDYiB+UvJzOt6bPE85KtaoSNM3B4lv8BgniOu7kue0aJfP8yUImIkAuPzyy81r/vjjj/Nmz9mwYUNhrgbtW/LYY4/pr776Ku/SJ4GZaIBC1cl8bINkwGOKrmmkLLVBcbJl2dUOh5WNPedkiaMaMjNRqjTazWNM5WP38JhaoAqlNJ+F5ROGL/aY0h63VXusB1/MRPWtvG2qpVhTaRXIuzy2slJzeZ5ylDb72mrPYer9enf2VFOJuyX5rLqVx/gAzEQAXHbZZeY1X7VqFW/2BE3TCoXAhg8fzgfUgs4555w+/y0JzESDHMrFThED0tu2Ac+u/bRFuN5x5RCeoxr62vjxIn5OIYc9by8pudif9DUtjkoGqyEzE4Tos5LHMN3npHy5/tTUj4nr/5QkjyklG224BoEfZkIvzm/YYctfRvRahOcoR5VdN+1yWCNCyySvtOXorQNaKnUFjysHVfqk7dQleazaRXM+eKwPwEwEwMSJE81rvnr1at7sOlSvo9YCYBdccIH++uuv8xR9ApgJF1DysTPFYPQBH5zK6JDQE1RIqrMt+mmey4De6Zd20KRB9aAkj0zv1TOnQA2hmSBDpFY1T9GX1PbYxTyWQxu0abnYa/b4wxImrOF39oQfZoIQA+h8W/7yku7dIUPLJL4uiS+nt+t55aSmE1lJLibRJ5M4l8caiOMeTRNDhfl5wx7bW8LATObxPgEzEQCXXnqpec1zuRxvdpW9e/c6Lvx17LHHhnKDtEaBmXAJ2pVTDErv8UGqBr0vBrpthdclQoW/i59J+lXT20p+yt/z86oFNYRmgqCloDxOpoJRyMbnKfnov2i5+HgyD8IcXEvlzUvX0xbD4/XstI/z49eDX2ZCbU1eaMtfTq2JcTy+HDRIq+VXXfQSlcjm8bVAm22J+F08n0xKOrm9NA/kroIyiaT482mhA7yvTEoqWbXOiofATATA+PHjzWvu9aDtpOCXVf3799c7Ojp4uqYGZsJFaJWAGJje4IOV58rHtjSyOkENqZkgaH4Ej3VZe92chOmXmRCD/hHi0/se2zHs2iF+zx3thFpp1UUvZZIX8dhaUdLpL6ryjcjc1HO0JJUf20dgJgLgoosuMq/5s88+y5tdgwp9Wf99ner444/Xd+7cydM2LTATLqN3TDlWrTCIui3axKvRnS7VEJuJwqS9XOw+Hu+KsrHdSjb6BX7MRvDLTBDiE/vDtmMwaenkj3hcNWiLcp5Hovf0hQv781gnqKnUP1IeSW4XlFrLd0cNAJiJADj//PPNa04DvldY52bUqxkzZvC0TQvMhEdoufhlYsD6s20Ac0n0DQh9pc+PWw9qiM2EQakyZifP04A2V5qzUi9+mokKO4MeVh3fHtAgLGIP2XJZ5Nbrg657EiNFvs08fyOi+SQBLAOVATMRAOeee655zdevX8+bXYEqhZYrPe5EAwYMaKrqopWAmfAQfd3kAUou9h0xcO2UDGZ1SeTbruSi1+uPTnbtYak2gZkg6DWSiH2B53Io2qTtPvq34fndwE8zod977wC14tyB1N56vz1Qi/MSJDmL0tKJiTymXmjgFznnqI2/9tgaQGGqSsBMBAAtwzSuOdV88IJ8Ps8HzrrV3t7O0zclMBM+QAO/lotOpHoGYiA7IBngqokmZD6qZaNfc7IcslZE7o00UZFL/Pxl3tcJIseDPKch2n2V968VWsEhzq1NrVyBlOuAOO5CL76NsNKVSHyGylnL5EaVR46WTj3Ij2PRA7x/rWip1FWSfAXRoE1Ghsc0Cu1oKnKnaNKlxChU0q+EuYlRnQyeM2BgJgJg7Nix5jXftGkTb3YF2trc+m/biBYuXMjTNyUwEz5DZkBpbxmr5GI3icHt/sKgmI910MBdUOHv0TXUpuTiN4p+ZzspQPXXBK3AKL5OimbEtVtduo5U3XIz/V1cu5+Kv9+hZuMX6mtuDHIiHnAITdBU0ombhFl6qLTB2C/Fn/9X/Hsiq6aS9wrDE9XvnfNJHhsiYCYC4MwzzzSv+ebNm3mzK1B1S+u/bSPKZDI8fVMCMwEAAN4AMxEAZ5xxhnnNt2zZwptdYdGiRXzgrFvLli3j6ZsSmAkAAPAGmIkAGD16tHnNt27dyptdgepXWP9tG5FXk0T9BmYCAAC8AWYiAEaOHGle89dee403uwKtwBg4cCAfPB1rxIgRek9PD0/flMBMAACAN8BMBMDpp59uXvNt27bxZteYNm0aHzwdizYH6yvATAAAgDfATATAqaeeal7zt956ize7xvbt2/VBgwbxAbRm0eZg3d3dPG3TAjMBAADeADMRANZtwL0uV71y5Uq9X79+fBCtKtocjHYb7UvATAAAgDfATATAySefbF7zd955hze7DhmKY445hg+kZUWbg+3bt4+naXpgJgAAwBtgJgJg2LBh5jXfs2cPb/aEN998U586dWrZSZlUenvChAme7hUSNDATAADgDTATATB06FDzmu/fv583ewqt8shms/rSpUv1mTNn6kuWLNHb2toKe3n0dWAmAADAG26LWB6us2bN4s9f4AHHHXecec3/GgbxsED3t/V+L93/AAAAGuSaiOXh2pe2mw4zgwcPNq95Z2cnbwYeQfe39X4v3f8AAAAahHYwNR+ukyZN4s9f4AHWeQuqqvJm4BF0f1vv99L9DwAAoEE+H7E8XEeNGsWfv8ADjjrqKPOaA/+g+9t6v5fufwAAAA0yUOhgxPKAfeONN/gzGLjM7NmzC5MfMUfFP+i+tt7npfue7n8AAAAu8FTE8pCdO3cufw4D0PTQfW29z0v3PQAAAJeYGrE8ZE877TRdURT+LAagaaH7me5r631euu8BAAC4xNAIe9Uxf/58/jwGoGmh+9l6f5fud7rvAQAAuEg6YnnYnnDCCfqOHTv4MxmApoPuY7qfrfd36X4HAADgMscK7YlYHrhnnXVWoVoiAM0K3b90H1vv69J9Tvc7AAAAD7gx0vuhW1iXf/DgQf6MBiD00H0rqStBovscAACAhyyPsIcvfbLDKw/QTND9KvlGgkT3NwAAAI85WujFCHsI08ZUCxYswCoPEGro/qT71LqRmkV0X9P9DQAAwAdOjEgMBYmW19F6fRS2AmGC7ke6LyXLP61Ggu5rAAAAPkKf4GyvPKyi0sQTJ07Ur7vuukI1RwjyU3Tf0f0nKZHNRfcxvpEAAIAAoclqvVZ5QFCTiO5bTLYEAICQQMvoUkKHIvYHNgSFTXSf0v2K5Z8AABBCqGIglSBeFWEVMyEoYNH9SPcl3Z+obAkAAE0C7bY4RuhioauF7oIgn3V1pHj/0X2I3T8BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABA3+H/AaJnb21xRjY5AAAAAElFTkSuQmCC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ7N9Pmt9kvm"
      },
      "source": [
        "This colab illustrates how to use the [OneTwo](https://github.com/google-deepmind/onetwo) library.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF_Z92hr2fhG"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWpY81IcArpB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A6jm6BDe8g_"
      },
      "outputs": [],
      "source": [
        "# Uncomment the following line and execute the cell to install OneTwo (if not already installed).\n",
        "#!pip install git+https://github.com/google-deepmind/onetwo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv6vzl2rZciy"
      },
      "source": [
        "# Overview\n",
        "\n",
        "One of the key principles behind the OneTwo library is to enable the creation of complex flows involving several calls to foundation models and possibly other tools.\n",
        "For ease of experimentation, it is important to easily change the backends or their configuration and run the same flow on two backends/configurations, e.g. when doing comparisons.\n",
        "\n",
        "The bottleneck is often the multiple RPC requests that need to happen. This makes fast iterations or experimenting on many examples slow and tedious. In order to reduce this bottleneck, there are two strategies that are implemented in the OneTwo library:\n",
        "1. **Caching**: The result of the calls to the models are cached, which enables one to very quickly replay a flow or an experiment which may have partially executed (e.g. failed in the middle of execution). For example, if you have a complex flow and want to add just one extra step, rerunning the whole thing amounts to reading everything from cache and only executing for real that one last step.\n",
        "1. **Asynchronous Execution**: While some of the model calls might need to be chained serially, there are many situations when you may want to execute some calls in parallel (e.g. talking to different backends, running an experiment on many examples, or having a step in your flow where several independent tasks are performed). A natural way to do that is to use asynchronous programming, or multi-threading.\n",
        "\n",
        "In this section, we provide an introduction that illustrates how to connect to and prompt a model using the basic OneTwo functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAJGlUJLNwT0"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGi9bnZRNzg5"
      },
      "outputs": [],
      "source": [
        "# At minimum, these are the libraries you will need to import to perform basic\n",
        "# operations with OneTwo.\n",
        "from onetwo import ot\n",
        "from onetwo.builtins import llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsvl1UcXECJb"
      },
      "outputs": [],
      "source": [
        "# These are some additional libraries that we will use in certain sections of\n",
        "# the colab.\n",
        "from collections.abc import Sequence\n",
        "import copy\n",
        "import IPython\n",
        "import os\n",
        "import pprint\n",
        "import re\n",
        "import textwrap\n",
        "from typing import Any\n",
        "\n",
        "from onetwo.agents import python_planning\n",
        "from onetwo.agents import react\n",
        "from onetwo.builtins import composables\n",
        "from onetwo.core import results\n",
        "from onetwo.evaluation import agent_evaluation\n",
        "from onetwo.stdlib.code_execution import python_execution_safe_subset\n",
        "from onetwo.stdlib.tool_use import llm_tool_use\n",
        "from onetwo.stdlib.tool_use import python_tool_use\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6soYEHIjZci_"
      },
      "source": [
        "## Connecting to a Model\n",
        "\n",
        "The Gemini, and OpenAI APIs provide an easy way to connect to a model. Select your preferred API/model in the drop-down and enter the corresponding API key (where relevant) in the text box below.\n",
        "\n",
        "OneTwo also supports connecting to other models such as Gemma models running locally or on a server. See Appendix A for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0HS107hVx3-"
      },
      "source": [
        "### Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akiM6TkoZu5p"
      },
      "outputs": [],
      "source": [
        "# @title {run:\"auto\"}\n",
        "\n",
        "# Select one of the LLM backend options here\n",
        "model_selection = 'Gemini API' # @param [ 'Gemini API', 'OpenAI API']\n",
        "\n",
        "# Boilerplate for conditional cell execution\n",
        "@IPython.core.magic.register_cell_magic('run_if')\n",
        "def run_if(line, cell):\n",
        "  if eval(line):\n",
        "     IPython.get_ipython().run_cell(cell)\n",
        "  else:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AN_zoMUrXs6"
      },
      "source": [
        "### Caching Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9qpS98lwlYw"
      },
      "source": [
        "Here we define some helper functions for loading/saving backend caches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHrn17vNxuzp"
      },
      "outputs": [],
      "source": [
        "# Here we define a location in which to store a cache of requests/replies for\n",
        "# each backend of interest, which we can use for speeding up running of the\n",
        "# colab if we re-run it (or make iterative modifications to it) in the future.\n",
        "# We will use a separate cache file for each backend.\n",
        "CACHE_BASE_PATH = '/tmp/onetwo_colab_backend_caches/tutorial'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUKMriyEpX9n"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLJ9Oc3DrZWM"
      },
      "outputs": [],
      "source": [
        "def get_model_name_for_cache(model_name: Any, temperature: float = 0.0) -\u003e str:\n",
        "  \"\"\"Returns a name to identify the model/temperature for caching.\"\"\"\n",
        "  if not isinstance(model_name, str):\n",
        "    model_name = str(model_name)\n",
        "  if match := re.search(r'\\.(.+)', model_name):\n",
        "    model_name = match[1]\n",
        "  if temperature \u003e 0:\n",
        "    temperature_str = str(temperature).replace('.', '_')\n",
        "    model_name = f'{model_name}_{temperature_str}'\n",
        "  return model_name\n",
        "\n",
        "def get_cache_path(model_name: Any, temperature: float = 0.0):\n",
        "  \"\"\"Returns the path for caching a given model (may be an enum or a string).\"\"\"\n",
        "  model_name_for_cache = get_model_name_for_cache(model_name, temperature)\n",
        "  return os.path.join(CACHE_BASE_PATH, f'{model_name_for_cache}.json')\n",
        "\n",
        "def load_backend_cache(backend: Any):\n",
        "  \"\"\"Checks if the cache file already exists, in which case we load it.\"\"\"\n",
        "  if os.path.exists(backend.cache_filename):\n",
        "    print(f'Loading cache from {backend.cache_filename}.')\n",
        "    backend.load_cache()\n",
        "    cache_length = len(backend._cache_handler._cache_data.values_by_key)\n",
        "    print(f'Loaded {cache_length} items.')\n",
        "  else:\n",
        "    print(f'Cache file does not exist: {backend.cache_filename}')\n",
        "\n",
        "def load_backend_caches(backends: Sequence[Any]):\n",
        "  \"\"\"Loads the caches of all the given backends.\"\"\"\n",
        "  for backend in backends:\n",
        "    load_backend_cache(backend)\n",
        "\n",
        "def save_backend_caches(backends: Sequence[Any]):\n",
        "  \"\"\"Saves the caches of all the given backends.\"\"\"\n",
        "  for backend in backends:\n",
        "    print(f'Saving cache to {backend.cache_filename}.')\n",
        "    backend.save_cache(overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVQ-lzf5j34Y"
      },
      "outputs": [],
      "source": [
        "# We will keep track of all the backends that we construct in this colab (as a\n",
        "# mapping of name to backend instance), to make it easy to load/save the caches.\n",
        "# Having a mapping of backends like this can also be convenient when doing\n",
        "# things constructing an experiment run that does a sweep over multiple LLMs.\n",
        "backends = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-uL_yLBZf2t"
      },
      "source": [
        "### Gemini API\n",
        "\n",
        "OneTwo can connect to publicly-hosted Gemini models via the Gemini API. If you have not used the Gemini API before, you will need to first create an account and API key following the instructions on https://ai.google.dev/. Then either copy-paste your API key into the text box, or store it in the 'GOOGLE_API_KEY' environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbNLcbPvZci_"
      },
      "outputs": [],
      "source": [
        "%%run_if model_selection == 'Gemini API'  # Execute cell only for 'Gemini API'\n",
        "\n",
        "from onetwo.backends import gemini_api\n",
        "\n",
        "# You can specify your API key either here or as an environment variable.\n",
        "api_key = \"\"  # @param {type: 'string'}\n",
        "\n",
        "if not api_key and 'GOOGLE_API_KEY' not in os.environ:\n",
        "  raise ValueError(\n",
        "      'The api key must be specified either here or in the environment.')\n",
        "\n",
        "# Create and register a connection to the default Gemini backend (Gemini Pro).\n",
        "backend = gemini_api.GeminiAPI(\n",
        "    api_key=api_key,\n",
        "    temperature=0.0,\n",
        "    cache_filename=get_cache_path('GEMINI_PRO', temperature=0.0),\n",
        ")\n",
        "backends['GEMINI_PRO'] = backend\n",
        "backend.register()\n",
        "print('Gemini API backend registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrpCvVxJZi9k"
      },
      "source": [
        "### OpenAI API\n",
        "\n",
        "OneTwo can connect to OpenAI models via the OpenAI API. If you have not used the OpenAI API before, you will need to first create an account and API key at https://platform.openai.com/signup. Then copy-paste your API key into the text box below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6o2jzmjHXIA"
      },
      "outputs": [],
      "source": [
        "%%run_if model_selection == 'OpenAI API'  # Execute cell only for 'OpenAI API'\n",
        "\n",
        "from onetwo.backends import openai_api\n",
        "\n",
        "# You can specify your API key either here or as an environment variable.\n",
        "api_key = \"\"  # @param {type: 'string'}\n",
        "\n",
        "if not api_key and 'OPENAI_API_KEY' not in os.environ:\n",
        "  raise ValueError(\n",
        "      'The api key must be specified either here or in the environment.')\n",
        "\n",
        "model_name = 'gpt-3.5-turbo'  # Specify the model of your choice.\n",
        "\n",
        "# Create and register a connection to the OpenAI backend.\n",
        "backend = openai_api.OpenAIAPI(\n",
        "    api_key=api_key,\n",
        "    model_name=model_name,\n",
        "    temperature=0.0,\n",
        "    cache_filename=get_cache_path(model_name, temperature=0.0),\n",
        ")\n",
        "backends[model_name] = backend\n",
        "backend.register()\n",
        "print('OpenAI API backend registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNGj_MPN4emn"
      },
      "source": [
        "### Saving and Loading Caches\n",
        "\n",
        "When creating a backend, one can associate a cache file to it, which allows one to store and retrieve previously executed requests. Note that when we created the LLM backend connections above, we already specified a location for loading/saving the cache, using the parameter `cache_filename`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ERDWDsqRdbe"
      },
      "source": [
        "The following initializes the cache if the file exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnN_ZaE64emo"
      },
      "outputs": [],
      "source": [
        "# This is a simple helper function that we defined earlier in the colab.\n",
        "load_backend_caches(backends.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bhwQihP8Zb3"
      },
      "source": [
        "We can then save the model cache to the associated file at any time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr7Oo-Ok7iAF"
      },
      "outputs": [],
      "source": [
        "# save_backend_caches(backends.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsofAL74Zci_"
      },
      "source": [
        "## LLM Built-ins and Executables\n",
        "\n",
        "OneTwo provides a number of built-in functions representing the basic operations one may want to perform on an LLM.\n",
        "\n",
        "- `llm.generate_text()` - Generate raw text.\n",
        "- `llm.instruct()` - Generate answer to instructions.\n",
        "- `llm.chat()` - Generate text in a multi-turn dialogue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nnzGJ8fT9AG"
      },
      "source": [
        "### `llm.generate_text()`\n",
        "\n",
        "The function `llm.generate_text()` asks the model to continue the given text. This works for both base models and instruction-tuned models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VXQvK2YZci_"
      },
      "outputs": [],
      "source": [
        "# Ask the model to continue the given text.\n",
        "e = llm.generate_text(\n",
        "    'Three not so well known cities in France are',\n",
        "    max_tokens=20,\n",
        ")\n",
        "print(ot.run(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tleOIzMZci_"
      },
      "source": [
        "Note that the LLM built-ins do not directly issue a request to the model. Instead, they return an `Executable` (the variable `e` in the above example), which can then be executed to produce the final result. The benefit of this two-step process is that one can define possibly complex execution flows in a natural pythonic way, and decouple the definition of the flow from the actual backends that are used to execute it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwYEKWRUhEJ5"
      },
      "source": [
        "### `llm.chat()`\n",
        "\n",
        "The function `llm.chat()` asks the model to continue a sequence of messages that are a back-and-forth between a user and the model. As illustrated in the example below, each `Message` consists of a role and some content. The exact behavior of `llm.chat()` is model-specific. By default, OneTwo creates a prompt by merging the messages and adding model-specific control tokens and role indicators. The prompt is then sent to `llm.generate.text()`. However, if the model supports `chat` API natively (e.g., like most of the OpenAI and Gemini models), `llm.chat` uses it directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSV0tkXwv39u"
      },
      "outputs": [],
      "source": [
        "from onetwo.core import content\n",
        "\n",
        "messages = [\n",
        "    content.Message(\n",
        "        role=content.PredefinedRole.USER,\n",
        "        content=(\n",
        "            'Pretend that you are Albert Einstein in 1911.\\n'\n",
        "            'Hi, my name is Peter. Who are you?'\n",
        "        ),\n",
        "    ),\n",
        "    content.Message(\n",
        "        role=content.PredefinedRole.MODEL,\n",
        "        content='Nice to meet you, Peter. My name is Albert.',\n",
        "    ),\n",
        "    content.Message(\n",
        "        role=content.PredefinedRole.USER,\n",
        "        content='Tell me more about yourself. Do you have a family and what is your job like?',\n",
        "    ),\n",
        "]\n",
        "e = llm.chat(messages, max_tokens=20)\n",
        "print(ot.run(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9g9iZp1iLjS"
      },
      "source": [
        "### `llm.instruct()`\n",
        "\n",
        "The function `llm.instruct()` asks the model to follow a certain instruction. This is different from `llm.generate_text()` in that the request is formatted such that it is clear that we want the instruction to be followed. The exact behavior of `llm.instruct()` is model-specific. By default OneTwo converts it into an `llm.chat()` request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uACg3J1ZUJsu"
      },
      "outputs": [],
      "source": [
        "# Issue a generate_text() request.\n",
        "e = llm.instruct('Write a 4 line poem about the Swiss Alps.', max_tokens=30)\n",
        "print(ot.run(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_iAv6afOH3E"
      },
      "source": [
        "### Printing actual formatted prompts sent to the model\n",
        "\n",
        "Often when using builtins like `llm.instruct` and `llm.chat` and debugging your code it may be important to print the arguments used in actual calls to the underlying model API (e.g., final formatted prompts or lists of messages). One simple way to do it is to swap (mock) the `llm.generate_text` (which often happens to be the very last step for many of the builtins) with a fake implementation that returns its input.\n",
        "\n",
        "Below we connect to the `gpt-3.5-turbo-instruct` OpenAI model. This model does not natively support the `chat` API. When we register this backend, `llm.chat` gets configured with the default implementation that formats the prompt and calls `llm.generate_text`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33xFxpD1OOgc"
      },
      "outputs": [],
      "source": [
        "%%run_if model_selection == 'OpenAI API'  # Execute cell only for 'OpenAI API'.\n",
        "\n",
        "new_backend = openai_api.OpenAIAPI(\n",
        "    api_key=api_key,\n",
        "    model_name='gpt-3.5-turbo-instruct',  # Does not support `chat` natively.\n",
        "    temperature=0.0,\n",
        ")\n",
        "new_backend.register()\n",
        "\n",
        "messages = [\n",
        "    content.Message(\n",
        "        role=content.PredefinedRole.USER,\n",
        "        content=(\n",
        "            'Hello! Who is your favourite writer?'\n",
        "        ),\n",
        "    ),\n",
        "    content.Message(\n",
        "        role=content.PredefinedRole.MODEL,\n",
        "        content='I guess one of the American novelists from the 1940s, e.g.',\n",
        "    ),\n",
        "]\n",
        "e = llm.chat(messages, max_tokens=20)\n",
        "\n",
        "# `llm.chat` formats the prompt and calls `llm.generate_text`.\n",
        "print(\n",
        "    f'llm.chat returned:\\n--\u003e|{ot.run(e)}|\u003c--',\n",
        "    end='\\n\\n',\n",
        ")\n",
        "\n",
        "def fake_generate_text(prompt: str | content.ChunkList, **kwargs):\n",
        "    return prompt\n",
        "\n",
        "llm.generate_text.configure(fake_generate_text)\n",
        "\n",
        "# Now `llm.generate_text` simply returns its input.\n",
        "print(f'llm.generate_text was called with prompt:\\n--\u003e|{ot.run(e)}|\u003c--')\n",
        "\n",
        "# Reset the builtins to their original state.\n",
        "backend.register()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWYnC670Zci_"
      },
      "source": [
        "## Composing Executables\n",
        "\n",
        "If we want to chain two successive calls, we can perform one after the other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zuef_kwvZci_"
      },
      "outputs": [],
      "source": [
        "result1 = ot.run(\n",
        "    llm.generate_text(\n",
        "        'Q: What is the southernmost city in France? A:', max_tokens=20\n",
        "    )\n",
        ")\n",
        "result2 = ot.run(\n",
        "    llm.generate_text(f'Q: Who is the mayor of {result1}? A:', max_tokens=20)\n",
        ")\n",
        "print(result1)\n",
        "print(result2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_kWQvNhZci_"
      },
      "source": [
        "But a better way is to create a new Executable that performs the all the desired operations and can then be executed on arbitrary backends. We do this by writing a function with the decorator `@ot.make_executable`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptzNx8XmZcjA"
      },
      "outputs": [],
      "source": [
        "@ot.make_executable\n",
        "async def f() -\u003e str:\n",
        "  result1 = await llm.generate_text(\n",
        "      'Q: What is the southernmost city in France? A:',\n",
        "      max_tokens=20,\n",
        "  )\n",
        "  print('Intermediate result:', result1)\n",
        "  result2 = await llm.generate_text(\n",
        "      f'Q: Who is the mayor of {result1}? A:',\n",
        "      max_tokens=20,\n",
        "  )\n",
        "  return result2\n",
        "\n",
        "result = ot.run(f())\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_sYw3yaZcjA"
      },
      "source": [
        "If we want to execute in parallel instead of executing serially, we can compose executables with `onetwo.parallel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P78BDUeZcjA"
      },
      "outputs": [],
      "source": [
        "# If instead of executing serially, we want to execute in parallel, we\n",
        "# can compose executables with `onetwo.parallel`.\n",
        "e1 = llm.generate_text(\n",
        "    'Q: What is the southernmost city in France? A:', max_tokens=20\n",
        ")\n",
        "e2 = llm.generate_text(\n",
        "    'Q: What is the southernmost city in Spain? A:', max_tokens=20\n",
        ")\n",
        "e = ot.parallel(e1, e2)\n",
        "result_list = ot.run(e)\n",
        "print(result_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWSlVHY1ZcjA"
      },
      "source": [
        "## Templates and Composables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BKl_h2lZcjA"
      },
      "source": [
        "The advantage of the LLM built-ins is that arbitrarily complex execution flows can be defined directly in the Python language. At the same time, this approach may not always be the best way to visualize the textual structure of the prompt. Therefore, we provide alternative techniques for defining and composing prompts, which tend to be more visual (\"what you see is what you get\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzVndXBBZcjA"
      },
      "source": [
        "One technique is to define and execute prompt templates using the Jinja2 syntax. We can create a Junja2 template using the built-in `composables.j()`. Note that this function again does not directly issue a call to a backend. Instead, it returns an executable that can be run on a certain model or can be composed with other executables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTP5uo-ZZcjA"
      },
      "outputs": [],
      "source": [
        "template = composables.j(\"\"\"\\\n",
        "What is the southernmost city in France? {{ generate_text(max_tokens=20) }}\n",
        "Who is its mayor? {{ generate_text(max_tokens=20) }}\n",
        "\"\"\")\n",
        "result = ot.run(template)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDu_vKvjZcjA"
      },
      "source": [
        "Another technique is to use `Composables`, which are variants of the LLM built-ins that can be concatenated into the prompt string using `+`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXjoSHb2ZcjA"
      },
      "outputs": [],
      "source": [
        "e = (\n",
        "    'What is the southernmost city in France? ' + composables.generate_text(max_tokens=20) +\n",
        "    '\\nWho is its mayor? ' + composables.generate_text(max_tokens=20)\n",
        ")\n",
        "result = ot.run(e)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JZM8UDFZcjA"
      },
      "source": [
        "## Prompt Variables\n",
        "\n",
        "Another useful technique is to use variables as part of the prompts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmEp1IdcZcjB"
      },
      "source": [
        "One way to use variables is via Python f-strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37CM70NjZcjB"
      },
      "outputs": [],
      "source": [
        "question = 'France'\n",
        "prompt1 = f'Q: What is the capital of {question}?\\nA:'\n",
        "res1 = ot.run(llm.generate_text(prompt1, max_tokens=10, stop=['Q:', '\\n\\n']))\n",
        "print(res1)\n",
        "prompt2 = f'Q: Who is the mayor of {res1}?\\nA:'\n",
        "res2 = ot.run(llm.generate_text(prompt2, max_tokens=10, stop=['Q:', '\\n\\n']))\n",
        "print(res2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLaedoVlZcjB"
      },
      "source": [
        "To make this code more reusable, we can compose the two executables into a single function that defines a new executable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf0kMwq9ZcjB"
      },
      "outputs": [],
      "source": [
        "@ot.make_executable\n",
        "async def capital_mayor(country: str) -\u003e str:\n",
        "  prompt = f'Q: What is the capital of {country}?\\nA:'\n",
        "  res = await llm.generate_text(prompt, max_tokens=10, stop=['Q:', '\\n\\n'])\n",
        "  prompt2 = f'Q: Who is the mayor of {res}?\\nA:'\n",
        "  return await llm.generate_text(prompt2, max_tokens=10, stop=['Q:', '\\n\\n'])\n",
        "\n",
        "print(ot.run(capital_mayor('France')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJJUTmLNZcjB"
      },
      "source": [
        "Another variant is to use variables in the Jinja 2 syntax. The following Jinja2 template is parameterized by the input variable `question`. In addition, we issue multiple requests and store their results in variables that can be referenced from within the prompt. The variables can also be retrieved from the resulting executable (`prompt`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgZava8qZcjB"
      },
      "outputs": [],
      "source": [
        "prompt = composables.j(\n",
        "    'Q: What is the capital of {{ question }}?\\n'\n",
        "    'A:{{ store(\"city\", generate_text(max_tokens=10, stop=[\"Q:\", \"\\n\\n\"])) }}\\n'\n",
        "    'Q: Who is the mayor of {{ __vars__.city }}?\\n'\n",
        "    'A:{{ store(\"mayor\", generate_text(max_tokens=10, stop=[\"Q:\", \"\\n\\n\"])) }}'\n",
        ")\n",
        "res = ot.run(prompt(question='France'))\n",
        "print(res)\n",
        "print(prompt['city'], prompt['mayor'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtpW_iuhZcjB"
      },
      "source": [
        "This above Jinja2 behavior can also be emulated using `composables`. We use `composables.f()` to format a string containing variables and we use `composables.store()` to store the results of LLM requests. As for Jinja2 variables, these variables can be retrieved from the resulting executable (`e`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ6ksMRLZcjB"
      },
      "outputs": [],
      "source": [
        "e = (\n",
        "    composables.f('Q: What is the capital of {question}?\\nA:') +\n",
        "    composables.store('city', composables.generate_text(max_tokens=10, stop=['Q:', '\\n\\n'])) +\n",
        "    composables.f('\\nQ: Who is the mayor of {city}?\\nA:') +\n",
        "    composables.store('mayor', composables.generate_text(max_tokens=10, stop=['Q:', '\\n\\n']))\n",
        ")\n",
        "res = ot.run(e(question='France'))\n",
        "print(res)\n",
        "print(e['city'], e['mayor'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-trZgnodt02P"
      },
      "source": [
        "## Sampling\n",
        "\n",
        "By default, the result for a given prompt execution is cached. This means that if you execute the same prompt more than once, a backend request is issued the only the first time and the cached result is returned for all subsquent executions.\n",
        "\n",
        "While this is useful, there are cases where we actually want to obtain different samples. OneTwo provides the function `repeat()` to achieve this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEM8zNMZlq2b"
      },
      "outputs": [],
      "source": [
        "prompt = (\n",
        "    'Here are three no-so-well-known cities in Switzerland:'\n",
        "    '1. Kilchberg, 2. Aesch, 3.'\n",
        ")\n",
        "\n",
        "# Make sure we set a non-zero temperature to avoid that the model returns\n",
        "# the same sample for each call.\n",
        "executable = llm.generate_text(prompt=prompt, max_tokens=20, temperature=0.5)\n",
        "\n",
        "# Create a list of containing 3 instances of the above executable.\n",
        "repeated_executable = ot.repeat(executable, 3)\n",
        "\n",
        "# Create an executable that executes the executables in the list in parallel.\n",
        "parallel_executable = ot.parallel(*repeated_executable)\n",
        "\n",
        "print(ot.run(parallel_executable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nrba0fFzY8k"
      },
      "source": [
        "Note that `repeat()` takes as an argument the `executable` and returns a list (`repeated_executable`) containing 3 instances of it. We then use `parallel` to turn this list into the single executable `parallel_executable`, which executes all elements in parallel.\n",
        "\n",
        "Also note that we set temperature to 0.5 in `generate_text()` to avoid that the model returns the same sample for each call.\n",
        "\n",
        "If we now ask for 6 samples, the first 3 will be drawn from the cache while the next 3 will be generated by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4tkwZuT1q8s"
      },
      "outputs": [],
      "source": [
        "repeated_executable = ot.repeat(executable, 6)\n",
        "parallel_executable = ot.parallel(*repeated_executable)\n",
        "\n",
        "print(ot.run(parallel_executable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djgz2cId2MpO"
      },
      "source": [
        "If we want to get 2 more samples, we can do this by providing the `start_index` as an additional argument to `repeat()`. In the code below, we use `start_index=6`, which means that we do not want to retrieve the first 6 results of the executable from the cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sCq71bZ2q_Q"
      },
      "outputs": [],
      "source": [
        "repeated_executable = ot.repeat(executable, 2, start_index=6)\n",
        "parallel_executable = ot.parallel(*repeated_executable)\n",
        "\n",
        "print(ot.run(parallel_executable))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0cpbigTZcjC"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "As you tweak your prompt or prompting strategy, it is important to use an evaluation process based on hard numbers and not just anecdotal evidence.\n",
        "OneTwo comes with an evaluation script to facilitate this process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQum4Gmkq-Kg"
      },
      "source": [
        "### Manual Comparison\n",
        "\n",
        "Our eval script can evaluate a prompting strategy by manually comparing the answers to the golden data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxYTdfbm2GwJ"
      },
      "outputs": [],
      "source": [
        "# Golden dataset to evaluate our prompting strategy.\n",
        "dataset = [\n",
        "    {'question': 'There are 100 people in a room. 55 are women and 70 are married. If 30 of the women are married, how many unmarried men are there?', 'answer': '15'},\n",
        "    {'question': 'A farmer has 12 sheep and 6 cows. How many more sheep than cows does he have?', 'answer': '6'},\n",
        "    {'question': 'A train travels 240 miles in 3 hours. What is its average speed in miles per hour?', 'answer': '80'},\n",
        "    {'question': 'A rectangular garden is 12 meters long and 8 meters wide. What is its perimeter?', 'answer': '40'},\n",
        "    {'question': 'If x + y = 10 and x - y = 2, what is the value of x?', 'answer': '6'},\n",
        "    {'question': 'A store sells apples for $0.50 each and oranges for $0.75 each. If I buy 5 apples and 3 oranges, how much will I spend?', 'answer': '4.75'},\n",
        "    {'question': 'A circle has a radius of 5 cm. What is its circumference in centimeters?', 'answer': '10*pi'},\n",
        "    {'question': 'A cube has a volume of 27 cubic feet. What is the length of one side of the cube in feet?', 'answer': '3'},\n",
        "    {'question': 'If 2^x = 16, what is the value of x?', 'answer': '4'},\n",
        "    {'question': 'What is the sum of the first 10 positive odd numbers?', 'answer': '100'}\n",
        "]\n",
        "\n",
        "# Define a simple strategy that passes the question directly to the model.\n",
        "@ot.make_executable\n",
        "async def strategy(question, **_):\n",
        "  answer = await llm.generate_text(\n",
        "      prompt=f'Question: {question}\\nFinal answer (formula or number):',\n",
        "      stop=['Question:', '\\n'],\n",
        "  )\n",
        "  return answer.strip()\n",
        "\n",
        "# Define a simple metric function that checks whether the correct answer is part\n",
        "# of the model output.\n",
        "def metric_fn(answer, example):\n",
        "  correct = str(example['answer']) in answer\n",
        "  extra_info = {}\n",
        "  if not correct:\n",
        "    index = hash(example['question'])\n",
        "    extra_info = {index: {\n",
        "        'question': example['question'][0:30] + '...',\n",
        "        'golden': example['answer'],\n",
        "        'answer': answer,\n",
        "    }}\n",
        "  return float(correct), extra_info\n",
        "\n",
        "# We run the evaluation on the dataset.\n",
        "time_elapsed, avg_metric, aggr_info = ot.evaluate(\n",
        "    strategy=strategy,\n",
        "    examples=dataset,\n",
        "    critic=metric_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiL6Z_CrNHxk"
      },
      "outputs": [],
      "source": [
        "# We can look at the cases where the model got a wrong answer.\n",
        "for v in aggr_info.values():\n",
        "  print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hYBlp1VIAJ7"
      },
      "source": [
        "### Using an LLM Critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XEF_qrEeEP0"
      },
      "source": [
        "In some cases, manual comparison to the Golden is difficult. For instance, consider the example above where the golden answer is `10*pi` while the model may provide an answer such as `31.4 cm`.\n",
        "\n",
        "The same holds for the dataset below, where the answer to the questions is not a fixed number or string. In such cases, the evaluation script allows one to use an LLM to judge whether or not the provided answer is equivalent to the golden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4A12d1OMz_"
      },
      "outputs": [],
      "source": [
        "# We create another dataset of questions that do not necessarily have a fixed\n",
        "# definite answer.\n",
        "dataset = [\n",
        "    {\n",
        "        'question': 'Who developed the TCP/IP protocol?',\n",
        "        'golden_answer': 'Bob Kahn and Vint Cerf',\n",
        "    },\n",
        "    {\n",
        "        'question': 'What date was the declaration of independence signed (not written)?',\n",
        "        'golden_answer': '8/2/1776'\n",
        "    },\n",
        "    {\n",
        "        'question': 'How big is the area of a triangle with base a and height h',\n",
        "        'golden_answer': '1/2 * ah',\n",
        "    },\n",
        "    {\n",
        "        'question': 'Which countried border Guatemala?',\n",
        "        'golden_answer': 'Mexico, Belize, Honduras, El Salvador',\n",
        "    },\n",
        "    {\n",
        "        'question': 'How do vaccines work?',\n",
        "        'golden_answer': (\n",
        "            'Vaccines contain weakened or inactive pathogens that stimulate the'\n",
        "            ' immune system to produce antibodies, which then protect against'\n",
        "            ' future infections from the same pathogen.'\n",
        "        ),\n",
        "    },\n",
        "]\n",
        "\n",
        "# Define a simple strategy that passes the question directly to the model.\n",
        "@ot.make_executable\n",
        "async def strategy(question, **_):\n",
        "  answer = await llm.generate_text(\n",
        "      prompt=f'Question: {question} ?\\nAnswer (concise):',\n",
        "      stop=['Question:'],\n",
        "      max_tokens=500,\n",
        "  )\n",
        "  return answer.strip()\n",
        "\n",
        "  # We run the evaluation on the dataset using an LLM critic.\n",
        "time_elapsed, total_votes, aggr_info = ot.evaluate(\n",
        "    strategy=strategy,\n",
        "    critic=ot.naive_evaluation_critic,\n",
        "    examples=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nESW4fQ0hnhu"
      },
      "outputs": [],
      "source": [
        "# This prints the critic prompt that was used.\n",
        "print(list(aggr_info.values())[0]['critic_prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBN4nc8eIfMt"
      },
      "outputs": [],
      "source": [
        "# This prints some debug information about the critic's judgments.\n",
        "for k, v in aggr_info.items():\n",
        "  print('----------')\n",
        "  print(f'Question: {k}')\n",
        "  print(f'   - Golden: {v[\"golden_answer\"]}')\n",
        "  print(f'   - Answer: {v[\"candidate_answer\"]}')\n",
        "  print(f'   - Correct: {v[\"answer_is_correct\"]}')\n",
        "  print(f'   - Reason: {v[\"reason\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J71bgSVypamm"
      },
      "source": [
        "## Agents and Tool Use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5rqq_U9bfk2"
      },
      "source": [
        "The same low-level primitives illustrated above can also be used to encapsulate generic higher-level strategies into reusable building blocks, which can in turn be composed to build more complex custom solutions.\n",
        "\n",
        "In this section we will illustrate two higher-level strategies that are available as off-the-shelf components in OneTwo, both targeting multi-step tool use:\n",
        "\n",
        "\n",
        "1.   `ReActAgent`\n",
        "2.   `PythonPlanningAgent`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuF-dxzpgZi1"
      },
      "source": [
        "Both of these strategies take the form of an \"agent\", which we define as a strategy that converts inputs to outputs by way of a series of repeated updates to an internal state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3pW2gYu2zIU"
      },
      "source": [
        "### ReAct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdCZIRuNeymM"
      },
      "source": [
        "The `ReActAgent` is based on the \"ReAct\" strategy presented in https://arxiv.org/abs/2210.03629.\n",
        "\n",
        "In this strategy, we present the LLM with a list of tool descriptions with invocation examples, and then iteratively prompt the LLM to output a sequence of steps, each of which consists of a \"thought\", an \"action\" and an \"observation\". The \"thought\" and \"action\" are output by the LLM directly. At each step, we programmatically parse the LLM-generated \"action\" string, perform the corresponding tool call, and then use the result of that tool call as the \"observation\" that is included in the LLM prompt in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgH--0of3J2n"
      },
      "source": [
        "#### Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss57Hu7NgPgP"
      },
      "source": [
        "As a first step, we will prepare a list of tools that we want to make available to the LLM.\n",
        "\n",
        "In this case, we will use two tools:\n",
        "\n",
        "*   **Python:** Tool for executing a program in a Python sandbox (e.g., for performing calculations).\n",
        "*   **Search:** Tool for retrieving snippets from Google Search.\n",
        "\n",
        "In addition, we provide one more \"tool\", which is actually just a means for the LLM to indicate when it is ready to return the final answer:\n",
        "\n",
        "*   **Finish:** Simple \"tool\" that the LLM uses to indicate when it is ready to return the final answer.\n",
        "\n",
        "For each tool, we provide a tool name, description, and usage example. These will all be included in the prompt that is shown to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j9Hwvs_TFO_"
      },
      "outputs": [],
      "source": [
        "python_sandbox_class = python_execution_safe_subset.PythonSandboxSafeSubset\n",
        "python_sandbox_factory = python_execution_safe_subset.PythonSandboxSafeSubsetFactory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JczX5xa3nm7"
      },
      "outputs": [],
      "source": [
        "# Python tool for executing a program in a Python sandbox.\n",
        "PYTHON_EXAMPLE = textwrap.dedent(\"\"\"\\\n",
        "  Python(\"1 + 1\") returns \"2\".\n",
        "  We can also run multiple lines of code like this:\n",
        "  ```yaml\n",
        "  Python:\n",
        "    request: |\n",
        "      a = []\n",
        "      a.append(1)\n",
        "      a.append(2)\n",
        "      a\n",
        "  ```\n",
        "  returns [1, 2].\"\"\")\n",
        "\n",
        "# Here we show the simplest case of a stateless Python tool. If we don't need\n",
        "# to carry variable state over from one call to another, we can just create a\n",
        "# fresh sandbox on each invocation of the Python tool.\n",
        "async def run_stateless_python(request: str) -\u003e str:\n",
        "  temporary_sandbox = python_sandbox_class()\n",
        "  async with temporary_sandbox.start() as temporary_sandbox:\n",
        "    result = await temporary_sandbox.run(request)\n",
        "    return str(result)\n",
        "\n",
        "python_tool = llm_tool_use.Tool(\n",
        "    name='Python',\n",
        "    function=run_stateless_python,\n",
        "    description='Python interpreter. Can be used as a calculator or to execute any Python code. Returns the result of execution.',\n",
        "    example=PYTHON_EXAMPLE,\n",
        "    color='plum',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zlnAB9I3oe_"
      },
      "outputs": [],
      "source": [
        "# In the open source environment, there are a number of commercial services\n",
        "# available for accessing web search. If you do not have web search connectivity\n",
        "# set up yet, you can start exploring the OneTwo agent strategies using the\n",
        "# following simple mock Search tool that returns hard-coded responses.\n",
        "# When using the agent for real, you can replace this with a function that calls\n",
        "# a real search engine, or that retrieves relevant passages from an indexed\n",
        "# corpus.\n",
        "def mock_search(query: str) -\u003e str:\n",
        "  response_by_query = {\n",
        "      'capital of France': 'Paris',\n",
        "      'population of Tuebingen': 'Tübingen 91,877 Population [2021]',\n",
        "      'population of Tübingen': 'Tübingen 91,877 Population [2021]',\n",
        "      'population Tuebingen': 'Tübingen 91,877 Population [2021]',\n",
        "      'population Tübingen': 'Tübingen 91,877 Population [2021]',\n",
        "      'population of Zuerich': '402,762 (2017)',\n",
        "      'population of Zurich': '402,762 (2017)',\n",
        "      'population of Zürich': '402,762 (2017)',\n",
        "      'population Zuerich': '402,762 (2017)',\n",
        "      'population Zurich': '402,762 (2017)',\n",
        "      'population Zürich': '402,762 (2017)',\n",
        "      'first president of the United States': 'George Washington',\n",
        "      'who was the first president of the United States?': 'George Washington',\n",
        "      'wife of George Washington': 'Martha Washington',\n",
        "      'who was the wife of George Washington?': 'Martha Washington',\n",
        "      'Frozen box office': '$1.280 billion',\n",
        "      'Frozen box office earnings': '$1.280 billion',\n",
        "      'Frozen movie box office earnings': '$1.280 billion',\n",
        "      'box office Frozen': '$1.280 billion',\n",
        "      'box office for Frozen': '$1.280 billion',\n",
        "      'box office of Frozen': '$1.280 billion',\n",
        "      'box office earnings of Frozen': '$1.280 billion',\n",
        "      'box office revenue Frozen': '$1.280 billion',\n",
        "      'how much did Frozen make at the box office?': '$1.280 billion',\n",
        "      'Lion King box office': '1.663 billion USD',\n",
        "      'Lion King box office earnings': '1.663 billion USD',\n",
        "      'Lion King movie box office earnings': '1.663 billion USD',\n",
        "      'box office Lion King': '1.663 billion USD',\n",
        "      'box office for Lion King': '1.663 billion USD',\n",
        "      'box office of Lion King': '1.663 billion USD',\n",
        "      'box office earnings of Lion King': '1.663 billion USD',\n",
        "      'box office revenue Lion King': '1.663 billion USD',\n",
        "      'how much did Lion King make at the box office?': '1.663 billion USD',\n",
        "      'Titanic box office': 'worldwide theatrical total = $2.264 billion',\n",
        "      'Titanic box office earnings': 'worldwide theatrical total = $2.264 billion',\n",
        "      'Titanic movie box office earnings': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office for Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office of Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office earnings of Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'box office revenue Titanic': 'worldwide theatrical total = $2.264 billion',\n",
        "      'how much did Titanic make at the box office?': 'worldwide theatrical total = $2.264 billion',\n",
        "  }\n",
        "  # Normalize capitalization.\n",
        "  response_by_query = {k.lower(): v for k, v in response_by_query.items()}\n",
        "  query = query.lower()\n",
        "  return response_by_query.get(query, 'No results.')\n",
        "\n",
        "search_tool = llm_tool_use.Tool(\n",
        "    name='Search',\n",
        "    function=mock_search,\n",
        "    description='Search engine. Returns a relevant snippet or answer to query.',\n",
        "    example=textwrap.dedent(\"Search('capital of France')  # returns 'Paris'\"),\n",
        "    color='darkseagreen',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QWTDoRa5q0X"
      },
      "outputs": [],
      "source": [
        "# The \"Finish\" function provides the LLM with a way of indicating when it is\n",
        "# ready to return a final answer. E.g., \"Finish('USA')\" returns 'USA'.\n",
        "finish_tool = llm_tool_use.Tool(\n",
        "    name='Finish',\n",
        "    function=lambda x: x,\n",
        "    description='Function for returning the final answer.',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD03FWpki1dT"
      },
      "outputs": [],
      "source": [
        "tools = [python_tool, search_tool, finish_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa-Jv6Sh3Gpm"
      },
      "source": [
        "#### Invoke ReActAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNioHChyireA"
      },
      "source": [
        "Once we've set up the tool handler, constructing a `ReActAgent` and executing it on a question is just a matter of a few lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9HlNTj3YkEz"
      },
      "outputs": [],
      "source": [
        "react_agent = react.ReActAgent(\n",
        "    exemplars=react.REACT_FEWSHOTS,\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        tools=tools,\n",
        "    ),\n",
        "    max_steps=10,\n",
        "    stop_prefix='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "852PwsNyjSGE"
      },
      "source": [
        "In the simplest usage, we can treat the `ReActAgent` as a black box -- i.e., as just a function that takes a question as input and then returns the answer. We can do this quite literally, as the `Agent` class qualifies as a `Callable`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZZ3iBXpYkE0"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer = ot.run(react_agent(inputs=question))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um2O6qsY3RJX"
      },
      "source": [
        "#### Inspect ReActAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkoM9zRn6Y6Q"
      },
      "source": [
        "##### Inspect steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6o9rkU_jVL-"
      },
      "source": [
        "If we want to see what is going on under the hood, there are multiple ways to do this.\n",
        "\n",
        "One simple way is to specify `return_final_state=True` when calling the agent. When we do this, we now receive a `final_state` object as return value, alongside the `answer` itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BemcJB9r581N"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer, final_state = ot.run(react_agent(inputs=question, return_final_state=True))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZqWwYatkMJc"
      },
      "source": [
        "If we print the final state, we can see the series of steps that the agent took in determining the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEQRNAMe6EMr"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx-iD6oC6dPs"
      },
      "source": [
        "##### Inspect detailed trace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj4Qvz0XkYrL"
      },
      "source": [
        "For even more details, we can specify `enable_tracing=True` in the call to `ot.run` to receive a detailed execution trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf3TIhce6lYR"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer, trace = ot.run(react_agent(inputs=question), enable_tracing=True)\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBNY3LVtk1p8"
      },
      "source": [
        "If we print the execution trace, we can see the exact series of requests that were sent to the LLM, along with the LLM's replies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPVy9Ccc6zmw"
      },
      "outputs": [],
      "source": [
        "print(results.format_result(trace, color=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9xtKbVBL9tG"
      },
      "source": [
        "We can also render the trace as interactive HTML block, where we can explore the full hierarchy of the prompting strategy, from the top-level agent down to the LLM and tool calls. Try clicking on the stage names to expand/collapse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soi784rGL9tH"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(trace))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIfWTPWR20ov"
      },
      "source": [
        "### Python Planning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUjphg0rpFth"
      },
      "source": [
        "The `PythonPlanningAgent` is inspired by various various research in Python-based tool orchestration, such as ViperGPT (https://arxiv.org/pdf/2303.08128.pdf) and AdaPlanner (https://arxiv.org/pdf/2305.16653.pdf).\n",
        "\n",
        "In this strategy, we present the LLM with a list of tool descriptions with invocation examples, and then iteratively prompt the LLM to output a sequence of steps, each of which consists of a Python code block, with \"thoughts\", where relevant, in the form of code comments. At each step, we execute the LLM-generated code in a Python sandbox that provides access to the relevant tools via predefined functions. We then take everything that the code writes to stdout, and we include that in the LLM prompt in the next step, similarly to how we did with the \"observation\" in the ReAct strategy.\n",
        "\n",
        "While the `ReActAgent` performs exactly one tool call in each step, the `PythonPlanningAgent` can potentially make multiple tool calls from a single code block, and can include other control structures like loops and if-statements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poM2x5ZZ3X0u"
      },
      "source": [
        "#### Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xspmdmg_4ook"
      },
      "source": [
        "Similarly to what we did for ReAct, we will again start by configuring the tools that we want to make available to the `PythonPlanningAgent`.\n",
        "\n",
        "The way we register the tools is very similar to before. One thing you might have noticed in the syntax for the `ReActAgent` was that we specified the list of tools as part of a `PythonToolUseEnvironmentConfig`. While for `ReActAgent`, we were treating the `PythonToolUseEnvironment` as basically just a class that manages a set of tools and provides a uniform way to call them, the `PythonToolUseEnvironment` actually contains quite a bit more functionality than that, including functionality to allow the tools to be called from within a Python sandbox, and to create and manage Python sandboxes on demand. In the `PythonPlanningAgent`, we will use this full range of functionality, as we orchestrate the tool use via execution of blocks of Python code.\n",
        "\n",
        "Note that for security reasons, it is important to always use a well-protected sandbox when automatically executing code that was generated by an LLM, similarly to how you would avoid any unprotected automatic execution of code that was provided by an untrusted user. The main idea of the sandbox is that we don't want to allow the LLM-generated program to directly read/write files or directly perform RPCs or import arbitrary libraries. Instead, we will provide an explicit allow-list of libraries to be imported and functions that can be called, which will include all of the tools from the tool handler.\n",
        "\n",
        "In this case, we will register two tools:\n",
        "\n",
        "*   **search:** Tool for retrieving snippets from Google Search (same as in the ReAct example).\n",
        "*   **firstnumber:** Simple function for extracting the first number from a block of text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m96l9fj4Hu_"
      },
      "outputs": [],
      "source": [
        "# Register a first tool called 'search' similar to the one used in ReActAgent.\n",
        "# For the purposes of this colab, it just returns hard-coded responses. (When\n",
        "# using the agent for real, you can replace this with a function that calls\n",
        "# a real search engine, or that retrieves relevant passages from an indexed\n",
        "# corpus.)\n",
        "search_tool = llm_tool_use.Tool(\n",
        "    name='search',\n",
        "    function=mock_search,\n",
        "    description='Search engine. Returns a relevant snippet or answer to query.',\n",
        "    example=textwrap.dedent(\"search('capital of France')  # returns 'Paris'\"),\n",
        "    color='darkseagreen',\n",
        ")\n",
        "\n",
        "# Register a second tool called 'firstnumber' which is just a simple Python\n",
        "# function that we define here, for extracting the first number from a block of\n",
        "# text.\n",
        "def firstnumber(x):\n",
        "  matches = re.match(r'[^\\d]*([\\d\\.,]+).*', str(x).replace(',', ''))\n",
        "  if matches:\n",
        "    try:\n",
        "      return float(matches.group(1))\n",
        "    except Exception as e:\n",
        "      return f'Error: could not parse {x} as a number ({e})'\n",
        "  else:\n",
        "    return f'Error: could not parse {x} as a number'\n",
        "\n",
        "first_number_tool = llm_tool_use.Tool(\n",
        "    name='firstnumber',\n",
        "    function=firstnumber,\n",
        "    description='Extracts the first number in a string.',\n",
        "    example=\"firstnumber('it is 1,203m high')  # return 1203.0 as a float\",\n",
        ")\n",
        "\n",
        "tools = [search_tool, first_number_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh5LmoZq3cgT"
      },
      "source": [
        "#### Invoke PythonPlanningAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4hB6ztC8acK"
      },
      "source": [
        "Once we've set up the tool handler, constructing a `PythonPlanningAgent` and executing it on a question is again just a matter of a few lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSZlzLw2yFE5"
      },
      "outputs": [],
      "source": [
        "python_agent = python_planning.PythonPlanningAgent(\n",
        "    exemplars=python_planning.DEFAULT_PYTHON_PLANNING_EXEMPLARS,\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        sandbox_factory=python_sandbox_factory,\n",
        "        tools=tools,\n",
        "    ),\n",
        "    max_steps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Nscop98kIB"
      },
      "source": [
        "In the simplest usage, we can again simply treat the `PythonPlanningAgent` as just a function that takes a question as input and then returns the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P_ilWpByFE6"
      },
      "outputs": [],
      "source": [
        "question = 'Which movie had the larger box office: Frozen or Titanic?'\n",
        "answer = ot.run(python_agent(inputs=question))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7tW0FNH3gGo"
      },
      "source": [
        "#### Inspect PythonPlanningAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kOqBQib8XIN"
      },
      "source": [
        "##### Inspect steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRjw-w2S8vvU"
      },
      "source": [
        "The same options that we saw for inspecting the intermediate steps of a `ReActAgent` are available for `PythonPlanningAgent` as well.\n",
        "\n",
        "In particular, if we  specify `return_final_state=True` when calling the agent, we receive a `final_state` object as return value, alongside the `answer` itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX6e8sgZ8fGP"
      },
      "outputs": [],
      "source": [
        "question = 'Which movie had the larger box office: Frozen or Titanic?'\n",
        "answer, final_state = ot.run(python_agent(inputs=question, return_final_state=True))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMlIWt7d9JWr"
      },
      "source": [
        "If we print the final state, we can see the series of steps that the agent took in determining the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GsjxWX1861z"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3ABClO78ZNE"
      },
      "source": [
        "##### Inspect detailed trace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHYyWcIQ9S-o"
      },
      "source": [
        "For even more details, we can again specify `enable_tracing=True` in the call to `ot.run` to receive a detailed execution trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iONYreoFDvF"
      },
      "outputs": [],
      "source": [
        "question = 'Which movie had the larger box office: Frozen or Titanic?'\n",
        "answer, trace = ot.run(python_agent(inputs=question), enable_tracing=True)\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yk9oaT_9dbZ"
      },
      "source": [
        "If we print the execution trace, we can again see the exact series of requests that were sent to the LLM, along with the LLM's replies. Each of the tool calls also appears in the execution trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uzkzWpVFDvF"
      },
      "outputs": [],
      "source": [
        "print(results.format_result(trace, color=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_O2RT8NK3Cp"
      },
      "source": [
        "We can also again render the trace as interactive HTML block to view the full hierarchy of the prompting strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrRktccEBxlX"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(trace))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj47HGeV2m4C"
      },
      "source": [
        "# Agents and Tool Use: Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J4sKi0COHm0"
      },
      "source": [
        "## Additional Agent Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFiLfkuYRzhb"
      },
      "source": [
        "One thing you may have noticed in the above examples is how similar the syntax is for interacting with `ReActAgent` and `PythonPlanningAgent`. That actually is not a coincidence! Both of these strategies have been implemented as subclasses of a generic `Agent` interface.\n",
        "\n",
        "Strategies that are implemented in this way support a number of additional operations out-of-the-box, in addition to what you saw above.\n",
        "\n",
        "We will illustrate some of these operations using the `ReActAgent` example from above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nISq_RQOKZH"
      },
      "source": [
        "### Stream states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3ISgVqrRFZt"
      },
      "source": [
        "For a long-running agent, rather than running monolothically, we may alternatively choose to stream the sequence of agent states, so that we can potentially save progress as we go along or apply our own dynamic criteria for stopping.\n",
        "\n",
        "We can do this using `Agent.stream_states`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlRBDVYsOedL"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))\n",
        "\n",
        "state_iterator = react_agent.start_environment_and_stream_states(\n",
        "    initial_state=state0)\n",
        "\n",
        "state_trajectory = []\n",
        "with ot.safe_stream(state_iterator) as state_stream:\n",
        "  for state in state_stream:\n",
        "    print('State arrived!')\n",
        "    state_trajectory.append(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN5z58UMSrWK"
      },
      "source": [
        "We end up receiving one state for each step that the agent performed -- in this case, 4 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tOLl8LnO095"
      },
      "outputs": [],
      "source": [
        "len(state_trajectory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIZU-gO9S71B"
      },
      "source": [
        "If we inspect the first state, we can see that it is of the same form as the final state that we saw earlier, except that the `updates` list contains only the first step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haJgNI-sOwPD"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(state_trajectory[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjnmoRwNTH4U"
      },
      "source": [
        "If we inspect the second state, we can see that the `updates` list now contains two steps. And so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h_g_8OSO7q_"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(state_trajectory[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxbPwShLQTSk"
      },
      "source": [
        "### Invoke prompt template directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_0UNyYETRHn"
      },
      "source": [
        "If you ever want to debug the behavior of the prompt template that is used by the agent internally (e.g., if you customize the prompt and are iterating on debugging), you can also execute the prompt template standalone, which can be done as follows, using any of the intermediate states from the state trajectory retrieved above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvWPnl_3yoF9"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(react_agent.environment_config.tools, width=160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRzsyVM8xbUA"
      },
      "outputs": [],
      "source": [
        "react_agent.prompt = react.ReActPromptJ2(text=react.DEFAULT_REACT_PROMPT_TEXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86CIzMzcQYT0"
      },
      "outputs": [],
      "source": [
        "prompt_outputs, trace = ot.run(react_agent.prompt(\n",
        "        # These arguments are just imitating what is done in the `ReActAgent`\n",
        "        # implementation (more or less copy-pasted from `react.py`).\n",
        "        tools=react_agent.environment_config.tools,\n",
        "        exemplars=react_agent.exemplars,\n",
        "        stop_prefix=react_agent.stop_prefix,\n",
        "        stop_sequences=react_agent._get_stop_sequences(),\n",
        "        force_finish=False,\n",
        "        # Here we can manually specify any of the intermediate states from the\n",
        "        # state trajectory above to reproduce the behavior of the prompt\n",
        "        # template at that step.\n",
        "        #state=state_trajectory[1],\n",
        "        state=state0,\n",
        "    ),\n",
        "    enable_tracing=True)\n",
        "pprint.pprint(prompt_outputs, width=160)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS1UfsT1UqJQ"
      },
      "source": [
        "If we want to see the precise prompt that was sent to the LLM, we can print the detailed execution trace of the prompt template, in the same way we did earlier for the agent strategy as a whole."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ3NRnzwQ1F2"
      },
      "outputs": [],
      "source": [
        "print(results.format_result(trace, color=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzNqGKcxOOF8"
      },
      "source": [
        "### Stream updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBmv7bMcXpWD"
      },
      "source": [
        "Similarly to how we produced a stream of agent states using `Agent.stream_states`, we can alternatively produce a stream of state updates using `Agent.stream_updates`. The idea is very similar, except that each agent update contains just the new information that needs to be added to the previous state to create the new state. In the case of `ReActAgent`, the state update is represented as a `ReActStep` (the same data structure that we saw inside of the `final_state` earlier, for representing the individual steps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yj5CPggWRFl"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))\n",
        "\n",
        "# Request a stream of updates.\n",
        "update_iterator = react_agent.start_environment_and_stream_updates(\n",
        "    initial_state=state0)\n",
        "\n",
        "# Two things we could do with these updates:\n",
        "# (A) Gather them in a list / work with them directly.\n",
        "# (B) Add them to a previous state to create the next state.\n",
        "updates = []\n",
        "current_state = copy.deepcopy(state0)\n",
        "with ot.safe_stream(update_iterator) as state_stream:\n",
        "  for update in state_stream:\n",
        "    print('Update arrived!')\n",
        "    updates.append(update)\n",
        "    # Agent states can be updated using `+=` with a state update.\n",
        "    current_state += update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE44WxG2Yl8P"
      },
      "source": [
        "If we look at the list of updates, we can see that each update is one `ReActStep`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_khaEA8WwPz"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(updates, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txPC-CrsYu3j"
      },
      "source": [
        "By incrementally updating an initial state with each of the updates using `+=`, we can also reproduce the current state at any given step. Now that we have processed the full update stream, we can see that `current_state` is exactly the same as the `final_state` that we observed earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiOrlNS_W7go"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(current_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcCA2iTOOQ6a"
      },
      "source": [
        "### Stop / edit / resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNE0AmYFZS-C"
      },
      "source": [
        "Since the behavior of the agent at each step is fully determined by the contents of the agent state, we are free to directly manipulate any of these state objects and pass them back into the agent to see what the agent would have done in that scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJqUuGdRZ-go"
      },
      "source": [
        "As an example, let's use `stream_states` for just 2 steps and temporarily stop execution there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTQ6Ey6GOlOy"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))\n",
        "state_iterator = react_agent.start_environment_and_stream_states(\n",
        "    initial_state=state0,\n",
        "    max_steps=2)\n",
        "with ot.safe_stream(state_iterator) as state_stream:\n",
        "  state_trajectory = list(state_stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sr151AxaKsF"
      },
      "source": [
        "At this point, we can see that the agent had just finish calling the `Search` tool to get the `population of Zuerich`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbT57_VNU6q9"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(state_trajectory[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM70KMHHaY5Z"
      },
      "source": [
        "Let's try modifying the `observation` from the last step to see what would have happened if Google Search had returned a different snippet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwMfNI90VCoE"
      },
      "outputs": [],
      "source": [
        "state_trajectory[1].updates[-1].observation = 'Population of Zurich: 5 million people and growing!'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68xAaVaJas6M"
      },
      "source": [
        "Now let's resume execution from this modified state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvEVO925VhC2"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer, final_state = ot.run(react_agent(\n",
        "    inputs=question,\n",
        "    initial_state=state_trajectory[1],\n",
        "    return_final_state=True))\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJgXKk8Vaybz"
      },
      "source": [
        "If we look at the final trajectory, we can see the modified internal state, as well as the new sequence of steps the agent would have taken after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm3G7wlkV9vT"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na0I3zpcrnlQ"
      },
      "source": [
        "### Customize exemplars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo5JIVYJrqLh"
      },
      "source": [
        "One thing you might have noticed when we were instantiating the `ReActAgent` and `PythonPlanningAgent` was that we needed to provide a list of exemplars.\n",
        "\n",
        "```\n",
        "react_agent = react.ReActAgent(\n",
        "    exemplars=react.REACT_FEWSHOTS,          # \u003c== Exemplars\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        tools=tools,\n",
        "    ),\n",
        "    max_steps=10,\n",
        "    stop_prefix='')\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "python_agent = python_planning.PythonPlanningAgent(\n",
        "    exemplars=python_planning.DEFAULT_PYTHON_PLANNING_EXEMPLARS,         # \u003c== Exemplars\n",
        "    environment_config=python_tool_use.PythonToolUseEnvironmentConfig(\n",
        "        sandbox_factory=python_sandbox_factory,\n",
        "        tools=tools,\n",
        "    ),\n",
        "    max_steps=10)\n",
        "```\n",
        "\n",
        "So far we have just used a predefined default list of exemplars, which were compatible with the set of tools that we had configured. In practice, though, you may want to customize the list of exemplars, or even select them dynamically from some kind of larger exemplar pool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZp7JTfhs4EH"
      },
      "source": [
        "Let's take a look at the exemplars that we have been using so far for `ReActAgent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbxaLlb5s-Oo"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(react_agent.exemplars, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtgyUcNutMnW"
      },
      "source": [
        "One thing you might have noticed is that the format of these exemplars looks very similar to the `final_state` that is output when we execute the agent with `return_final_state = True`.\n",
        "\n",
        "This is again not a coincidence, and you can indeed directly reuse any state object output from a past run of the agent strategy as an exemplar in future runs, or you can construct them fully manually, or semi-automatically via the \"stop / edit / resume\" workflow shown earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36klZfYouRPq"
      },
      "source": [
        "Let's try harvesting a couple of trajectories output by our current `react_agent` and use those as exemplars in a new agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrtMoeL4ucdb"
      },
      "outputs": [],
      "source": [
        "q1 = 'What is the total population of Tuebingen and Zuerich?'\n",
        "answer, final_state_q1 = ot.run(react_agent(inputs=q1, return_final_state=True))\n",
        "answer\n",
        "# pprint.pprint(final_state_q1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAlRgUvLw5MS"
      },
      "outputs": [],
      "source": [
        "q2 = 'Who was the wife of the first president of the United States?'\n",
        "answer, final_state_q2 = ot.run(react_agent(inputs=q2, return_final_state=True))\n",
        "answer\n",
        "# pprint.pprint(final_state_q2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLJXbk-OwU4t"
      },
      "outputs": [],
      "source": [
        "react_agent2 = react.ReActAgent(\n",
        "    exemplars=[final_state_q1, final_state_q2],\n",
        "    environment_config=react_agent.environment_config,\n",
        "    max_steps=10,\n",
        "    stop_prefix='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEJ_2H8BzAPr"
      },
      "source": [
        "If we look at the exemplars of the new agent, we can see that these indeed correspond to the trajectories from the two questions that we presented to the first agent above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veSmLJtnuxU5"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(react_agent2.exemplars, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUAW8TYhzOso"
      },
      "source": [
        "Now let's try the new agent on a new question and see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQEF42dLudJ6"
      },
      "outputs": [],
      "source": [
        "question = 'Which movie had the larger box office: Frozen or Lion King?'\n",
        "(answer, final_state), trace = ot.run(react_agent2(inputs=question, return_final_state=True), enable_tracing=True)\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdmFmd1DxnCj"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(final_state, width=140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq_3bIt-zSgb"
      },
      "source": [
        "If we look at the prompt that `react_agent2` sent to the LLM, we can see that the exemplars were now the ones that we specified, which were bootstrapped from the original `react_agent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXt64NrRvQM0"
      },
      "outputs": [],
      "source": [
        "print(results.format_result(trace.get_leaf_results()[0], color=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAqxD4A7dWT8"
      },
      "source": [
        "### Run step-by-step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WB-yqzWm0L6"
      },
      "source": [
        "As an alternative to iterating through a stream of updates, we can also run individual steps of the agent interactively using `Agent.sample_next_step`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx6arXo8diEU"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "env = python_tool_use.PythonToolUseEnvironment(config=config)\n",
        "\n",
        "# Since we will perform multiple operations on the same environment\n",
        "# actively, we start the environment manually here, rather than wrapping\n",
        "# everything in a context manager. (We will call `stop` manually later.)\n",
        "env = ot.run(env.start_unsafe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s4qStvGdsGb"
      },
      "outputs": [],
      "source": [
        "question = 'What is the total population of Tuebingen and Zuerich?'\n",
        "state0 = ot.run(react_agent.initialize_state(inputs=question))\n",
        "\n",
        "# Sample a single candidate for step 1).\n",
        "next_step_candidates = ot.run(\n",
        "    react_agent.sample_next_step(\n",
        "        state=state0, num_candidates=1, environment=env\n",
        "    )\n",
        ")\n",
        "pprint.pprint(next_step_candidates, width=160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2e_XUHdfd24"
      },
      "outputs": [],
      "source": [
        "update1 = next_step_candidates[0]\n",
        "state1 = state0 + next_step_candidates[0]\n",
        "\n",
        "# Sample a single candidate for step 2.\n",
        "next_step_candidates = ot.run(\n",
        "    react_agent.sample_next_step(\n",
        "        state=state1, num_candidates=1, environment=env\n",
        "    )\n",
        ")\n",
        "pprint.pprint(next_step_candidates, width=160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWA_aVVMwNHA"
      },
      "outputs": [],
      "source": [
        "# We need to stop the environment manually, since we started it manually\n",
        "# earlier.\n",
        "env.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtfBYHvf02dc"
      },
      "source": [
        "## PythonToolUseEnvironment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx5SCBFB1JBX"
      },
      "source": [
        "Although agents in OneTwo don't necessarily need to involve tool use, the two agents shown so far (`ReActAgent` and `PythonPlanningAgent`) both did take advantage of tool use and/or code execution capabilities, which were provided by a shared `PythonToolUseEnvironment` class.\n",
        "\n",
        "In this section, we will illustrate some additional operations that can be performed using the `PythonToolUseEnvironment`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz6t09Z0ri4v"
      },
      "source": [
        "### Invoke tools directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90u8RwzlrnH0"
      },
      "source": [
        "If you ever want to debug the behavior of the tools that are called by the agent internally, you can execute the tools standalone as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03moJ_f3tgan"
      },
      "source": [
        "First let's try executing the `Search` tool that we registered earlier with the `ReActAgent`. We can do this using `PythonToolUseEnvironment.run_tool`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3p2FjuOsS_N"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_tool(tool_name='Search', tool_args=['capital of France'], tool_kwargs={})\n",
        "  )\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3nCdvRctnk9"
      },
      "source": [
        "Now let's try executing the `Python` tool from the same `ReActAgent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTgvg459s6iR"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_tool(tool_name='Python', tool_args=['8849 - 8611'], tool_kwargs={})\n",
        "  )\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO_oNVzw0Guc"
      },
      "source": [
        "### Run code directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHoecBVVxF6N"
      },
      "source": [
        "To spawn a Python sandbox and run a block of code in it (potentially including calls to tools) the way that is done in `PythonPlanningAgent`, we can use `PythonToolUseEnvironment.run_code`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYP2hmTNu-7P"
      },
      "outputs": [],
      "source": [
        "config = python_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_code(\n",
        "          sandbox_state=tuple(),\n",
        "          code=textwrap.dedent(\"\"\"\\\n",
        "          search_result = search('population of Tübingen')\n",
        "          print('Search result: %s' % search_result)\n",
        "          population = firstnumber(search_result)\n",
        "          population\n",
        "          \"\"\"),\n",
        "      )\n",
        "  )\n",
        "pprint.pprint(result, width=160)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42gDq_8CxpB9"
      },
      "source": [
        "Note that `PythonToolUseEnvironment.run_code` is somewhat similar to running the \"Python tool\" from `ReActAgent`, but is more powerful, since the executed Python code can include calls to any of the tools registered in the `PythonToolUseEnvironment` (for example, the \"search\" and \"firstnumber\" tools invoked in the code snippet above). Note also that while the \"Python tool\" returns just a single string as its output, `run_code` returns a much more detailed `RunCodeResult` object, which contains, among other things, both the value of the final expression (in this case, the value of `population`) and all content that was written to `stdout`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc800ur92Dz2"
      },
      "source": [
        "One thing you may have noticed above is the `sandbox_state` parameter, which we set simply to an empty `tuple()`. The tuple provided in `sandbox_state` represents the sequence of code blocks that we expect to have been executed in the Python sandbox so far. In this case, we requested that the code be run in a fresh sandbox (i.e., one in which no code has been executed yet). For achieving the effect of a stateful sandbox, however, we can also specify a non-empty sequence of code blocks as the `sandbox_state`, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEizEIft0Uqa"
      },
      "outputs": [],
      "source": [
        "config = python_agent.environment_config\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(\n",
        "      env.run_code(\n",
        "          sandbox_state=(\n",
        "              \"search_result = 'Tübingen 91,877'\",\n",
        "          ),\n",
        "          code='firstnumber(search_result)',\n",
        "      )\n",
        "  )\n",
        "result.sandbox_result.final_expression_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcNohQjG3Vph"
      },
      "source": [
        "Notice that the code succeeded in running even though it included a reference to a variable `search_result` that was not directly defined in the given code block, since it was defined in one of the code blocks from the provided `sandbox_state`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrgHBKs64fTg"
      },
      "source": [
        "Notice also that the above `run_code` request succeeded even though we had never actually previously run the precise code block `\"search_result = 'Tübingen 91,877'\"` on this `PythonToolUseEnvironment` instance up till now. In cases like this, `PythonToolUseEnvironment` will automatically reconstruct Python sandbox instances on demand that match the requested state. This is convenient, for example, if you have a long-running `PythonPlanningAgent` strategy for which you would like to serialize the intermediate state, and then later deserialize the state and continue execution in a separate process, or if you were to apply a branching strategy like beam search over top of the `PythonPlanningAgent` trajectories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlY6w7Vf7GN_"
      },
      "source": [
        "In the simple case where you want to just execute a series of code blocks in a single stateful Python sandbox in a single process, you can simply pass in the list of code blocks executed so far in each call to `PythonToolUseEnvironment.run_code`, and the environment will automatically reuse the same sandbox for the whole series of calls, to avoid any unnecessary duplicate code execution.\n",
        "\n",
        "Here is an example of what that could look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjhmJUyY7JFK"
      },
      "outputs": [],
      "source": [
        "config = python_agent.environment_config\n",
        "code_blocks_to_execute = [\n",
        "    \"search_result = 'Tübingen 91,877'\",\n",
        "    'firstnumber(search_result)',\n",
        "]\n",
        "code_blocks_executed_so_far = []\n",
        "result_list = []\n",
        "\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  for code in code_blocks_to_execute:\n",
        "    result = ot.run(\n",
        "        env.run_code(\n",
        "            sandbox_state=tuple(code_blocks_executed_so_far),\n",
        "            code=code,\n",
        "        )\n",
        "    )\n",
        "    result_list.append(result)\n",
        "    code_blocks_executed_so_far.append(code)\n",
        "\n",
        "pprint.pprint(result_list, width=160)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTaLMNXSp0Nw"
      },
      "source": [
        "### Start environment manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IEDAPslshrj"
      },
      "source": [
        "When using a `PythonToolUseEnvironment`, we need to always \"start\" it first and then \"stop\" it onces we are done, so as to ensure that any background threads or other resources get cleaned up.\n",
        "\n",
        "As you may have noticed in the examples above, there are two different syntaxes available for starting and stopping the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVZozTtEtFG5"
      },
      "source": [
        "When implementing a prompting strategy for real, the recommended approach is to start the environment via a context manager, as shown below. In this approach, the environment is started when it is enters the context and then is automatically stopped when exiting the context. This is the \"safest\" syntax to use, as it ensures that the environment is always cleaned up at the end, even in cases where execution is interrupted due to an exception."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCVavsOqrEd0"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "# The environment will start when entering the context and stop when exiting.\n",
        "with python_tool_use.PythonToolUseEnvironment(config=config) as env:\n",
        "  result = ot.run(env.run_code(sandbox_state=tuple(), code='x = 1'))\n",
        "  result = ot.run(env.run_code(sandbox_state=('x = 1',), code='x * 2'))\n",
        "result.sandbox_result.final_expression_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brtFxXM9nbMJ"
      },
      "source": [
        "In cases where we will be performing a series of interactive operations on the same `PythonToolUseEnvironment` instance, though, it can be convenient to simply start the environment once at the beginning, and then just leaving it running indefinitely until we choose to manually stop it later, as shown below.\n",
        "\n",
        "Note that this approach is \"unsafe\" in the sense that if we forget to call `env.stop()` (or if that line fails to run due to an exception raised in an earlier part of the code), we could be left with some threads left running in the background, which would be undesirable in a long-running process. For the purposes of experimenting manually in colab, though, this approach is fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoRf89zxsJX6"
      },
      "outputs": [],
      "source": [
        "config = react_agent.environment_config\n",
        "env = python_tool_use.PythonToolUseEnvironment(config=config)\n",
        "\n",
        "# Here we start the environment manually.\n",
        "ot.run(env.start_unsafe())\n",
        "\n",
        "result = ot.run(env.run_code(sandbox_state=tuple(), code='x = 1'))\n",
        "result = ot.run(env.run_code(sandbox_state=('x = 1',), code='x * 2'))\n",
        "\n",
        "# Here we stop the environment manually.\n",
        "# (This could also be done in a separate cell.)\n",
        "env.stop()\n",
        "\n",
        "result.sandbox_result.final_expression_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-VJtIUNS0PC"
      },
      "source": [
        "## Agent Evals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk7w_Qzikw2v"
      },
      "source": [
        "For evaluating an agent strategy over a dataset, one option is to follow the same approach illustrated earlier in the \"Evals\" section, where the general-purpose `evaluation.evaluate` script provides a loose structure for the eval run, but where you define for yourself the format in which to output the evaluation results.\n",
        "\n",
        "As an alternative, though, we also provide an `agent_evaluation.evaluate` script, which is designed to be easy to use with agent strategies, and which automatically generates various detailed debug information out-of-the-box in a standardized format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCCWbuanm6He"
      },
      "source": [
        "In both cases, we would start by constructing a dataset consisting of a list or stream of examples, where each example is represented as a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtjACM5kTX_g"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    {'question': 'What is the total population of Tuebingen and Zuerich?',\n",
        "     'answer': 494639},\n",
        "    {'question': 'Which movie had the larger box office: Frozen or Titanic?',\n",
        "     'answer': 'Titanic'},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om74PtcSnUw0"
      },
      "source": [
        "When using the `agent_evaluation` script, we define our metric functions following the function signature shown below, where the function takes as input a target and prediction and returns a float value.\n",
        "\n",
        "In this example, we are providing just a simple, ordinary function, but it could also be an `async` function, e.g., for a metric based on an AI rater."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrS6UW9ZmIG"
      },
      "outputs": [],
      "source": [
        "def number_aware_accuracy(target: str | float | int, prediction: str) -\u003e float:\n",
        "  if isinstance(target, float) or isinstance(target, int):\n",
        "    # Numerical comparison: Ignore thousands separators, handle rounding error.\n",
        "    try:\n",
        "      prediction_as_float = float(prediction.replace(',', ''))\n",
        "      correct = (round(target - prediction_as_float, 8) == 0.0)\n",
        "    except ValueError:\n",
        "      correct = False\n",
        "  else:\n",
        "    # String comparison.\n",
        "    correct = (target == prediction)\n",
        "  return float(correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rah0K6TRodf-"
      },
      "source": [
        "You can then run the eval as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4swk9FMUJNF"
      },
      "outputs": [],
      "source": [
        "react_summary = agent_evaluation.evaluate(\n",
        "    strategy=react_agent,\n",
        "    examples=dataset,\n",
        "    metric_functions={'accuracy': number_aware_accuracy},\n",
        "    output_results=True,\n",
        "    output_results_debug=True,\n",
        "    output_final_states=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga51ECD0piD6"
      },
      "outputs": [],
      "source": [
        "print(f'\\nTime elapsed (seconds): {react_summary.timing.time_elapsed.total_seconds()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXImio46ptFe"
      },
      "source": [
        "The returned object is an `EvaluationSummary` object, which summarizes the aggregated eval results (metrics, counters, timing), along with various optional details for debugging purposes:\n",
        "* **results:** Brief summary of each example: Inputs, outputs, metric, etc.\n",
        "* **results_debug:** Detailed trace of each example.\n",
        "* **final_states:** Final state of the agent for each example (only relevant when the strategy is an agent).\n",
        "\n",
        "Try clicking on the various elements in the hierarchies to expand/collapse!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_JccsYZUUTW"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(ot.HTMLRenderer().render(react_summary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HWP6DhTw7Rc"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT052IG0ckbI"
      },
      "source": [
        "## Appendix A: Other Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFbbBl9Pcs3k"
      },
      "source": [
        "As of the initial release, we support connections to the following models (more will be added soon):\n",
        "- Gemini API\n",
        "- OpenAI API\n",
        "- Gemma running locally\n",
        "- Gemma running on a OneTwo model server\n",
        "\n",
        "In the Overview section, we illustrated how to connect to the Gemini and OpenAI APIs. Below, we illustrate how to setup and connect to Gemma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sesaZZtsglUb"
      },
      "source": [
        "### Gemma (running locally)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOxjsEVxx8FR"
      },
      "source": [
        "The [Gemma](https://ai.google.dev/gemma) family of open weights models ([GitHub](https://github.com/google-deepmind/gemma)) can be obtained from the following repositories:\n",
        "- Vertex Model Garden: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335\n",
        "- Kaggle: https://www.kaggle.com/models/google/gemma/\n",
        "- HuggingFace: https://huggingface.co/docs/transformers/en/model_doc/gemma\n",
        "\n",
        "As an example, below are the instructions for downloading a Gemma model from Kaggle. If you have not used Kaggle before, you will need to first create a Kaggle account and API key (a.k.a. API token) following the instructions on https://www.kaggle.com/docs/api. Then copy-paste your username and API key into the placeholders below.\n",
        "\n",
        "If you have not used Gemma on Kaggle before, you will also need to go to https://www.kaggle.com/models/google/gemma/ and click \"Request access\" to complete the consent form before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptIrpyV3_t7R"
      },
      "outputs": [],
      "source": [
        "# If you want to run this section, just change the below to `True`.\n",
        "# (We keep it disabled by default due to the kaggle dependency.)\n",
        "enable_gemma_local = False  # @param [\"True\", \"False\"] {type:\"raw\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLfR3pFzhDMN"
      },
      "outputs": [],
      "source": [
        "%%run_if enable_gemma_local\n",
        "\n",
        "# Install kaggle\n",
        "! pip install kaggle\n",
        "! pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWepgri9gptd"
      },
      "outputs": [],
      "source": [
        "%%run_if enable_gemma_local\n",
        "\n",
        "import kagglehub\n",
        "from kagglehub import auth\n",
        "\n",
        "auth.set_kaggle_credentials(\n",
        "    username='YOUR_KAGGLE_USERNAME', api_key='YOUR_KAGGLE_API_KEY'\n",
        ")\n",
        "kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHylR7QshOHB"
      },
      "outputs": [],
      "source": [
        "%%run_if enable_gemma_local\n",
        "\n",
        "VARIANT = '2b-it'  # @param ['2b', '2b-it', '7b', '7b-it'] {type:\"string\"}\n",
        "weights_dir = kagglehub.model_download(f'google/gemma/Flax/{VARIANT}')\n",
        "\n",
        "checkpoint_path = os.path.join(weights_dir, VARIANT)\n",
        "vocab_path = os.path.join(weights_dir, 'tokenizer.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD7WNFp_yzFb"
      },
      "source": [
        "Once you have downloaded a copy of the model, you can create a OneTwo backend using this model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vwlxrmDfNc8"
      },
      "outputs": [],
      "source": [
        "%%run_if enable_gemma_local\n",
        "\n",
        "from onetwo.backends import gemma_local\n",
        "\n",
        "backend = gemma_local.Gemma(\n",
        "    checkpoint_path=checkpoint_path, vocab_path=vocab_path\n",
        ")\n",
        "backend.register()\n",
        "print('Gemma local backend registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv8giCScg8k5"
      },
      "source": [
        "### Gemma (running on a separate server)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QvJLDTIzAcJ"
      },
      "source": [
        "While experimenting, you may not want to have to reload the model every time you change something in your code, so it can be convenient to set up the Gemma model to run in a separate process or even on a separate machine.\n",
        "\n",
        "We provide a model server script for that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzw2TkcIhja9"
      },
      "source": [
        "#### Server setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLYm4DaItG1J"
      },
      "source": [
        "In order to set up a server, you can use the `run_model_server.py` script. For example you can create a server that serves a Gemma model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3w18fFWhxSy"
      },
      "source": [
        "```shell\n",
        "CHECKPOINT_PATH=\"PATH_TO_THE_CHECKPOINT_DIR\"\n",
        "VOCAB_PATH=\"PATH_TO_THE_VOCAB_FILE\"\n",
        "\n",
        "python run_model_server.py \\\n",
        "  --backend_module=\"onetwo.backends.gemma_local\" \\\n",
        "  --backend_class=\"Gemma\" \\\n",
        "  --backend_params=\"{\\\"checkpoint_path\\\": \\\"$CHECKPOINT_PATH\\\", \\\"vocab_path\\\": \\\"$VOCAB_PATH\\\"}\" \\\n",
        "  --port=8888\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12trBlN7tUIb"
      },
      "source": [
        "You can also simply create your own python code to load a model locally or connect to a remote model and run as a simple web server using code that looks like:\n",
        "\n",
        "```python\n",
        "import uvicorn\n",
        "\n",
        "def main(args):\n",
        "  # Code to create the backend connection (local or remote).\n",
        "  backend = ...\n",
        "  backend.register()\n",
        "\n",
        "  # Starting a simple server piping requests to the registered backend.\n",
        "  uvicorn.run(\n",
        "      'onetwo.backends.model_server:ModelServer',\n",
        "      host='0.0.0.0',\n",
        "      port=8888,\n",
        "      factory=True,\n",
        "  )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m26cftOxhlYs"
      },
      "source": [
        "#### Client connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9EOqVN6uGiC"
      },
      "source": [
        "Once your server is set up, on the client side, you can connect to it using the onetwo_api module.\n",
        "\n",
        "If you have set up a server as described above, you can uncomment the code below and copy-paste your server hostname into the placeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tedsV2vmhnzP"
      },
      "outputs": [],
      "source": [
        "# from onetwo.backends import onetwo_api\n",
        "\n",
        "# backend = onetwo_api.OneTwoAPI(endpoint='http://SERVER_HOST_NAME:8888')\n",
        "# backend.register()\n",
        "# print('Gemma server backend registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-a8NwgirOKy"
      },
      "source": [
        "## Appendix B: Diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhAdiXolqCjy"
      },
      "outputs": [],
      "source": [
        "print('Cache summary:')\n",
        "for model_name, backend in backends.items():\n",
        "  print(f'* {model_name}: {backend.cache_filename}')\n",
        "  cache_length = len(backend._cache_handler._cache_data.values_by_key)\n",
        "  print(f'  * Cache contains {cache_length} items.')\n",
        "  print(f'  * Counters: {backend._cache_handler._cache_data.counters}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "WQum4Gmkq-Kg"
      ],
      "last_runtime": {
        "build_target": "//learning/brain/research/system12/onetwo/colabs:onetwo_colab",
        "kind": "private"
      },
      "name": "OneTwo - Tutorial",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
